Trained 200 at 57.62289071083069 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.899', '1.561', '0.314']Trained 204 at 97.72452449798584 seconds for [10000, 10000, 3802] epochs and to a loss of ['0.787', '1.239', '0.2']Trained 208 at 132.19752168655396 seconds for [10000, 10000, 574] epochs and to a loss of ['0.762', '1.149', '0.2']Trained 212 at 167.01675987243652 seconds for [10000, 10000, 931] epochs and to a loss of ['0.753', '1.111', '0.2']Trained 216 at 202.24873995780945 seconds for [10000, 10000, 1113] epochs and to a loss of ['0.749', '1.089', '0.2']Trained 220 at 237.79623794555664 seconds for [10000, 10000, 1247] epochs and to a loss of ['0.747', '0.847', '0.2']Trained 224 at 273.22293281555176 seconds for [10000, 10000, 1303] epochs and to a loss of ['0.746', '0.776', '0.2']Trained 228 at 309.0472662448883 seconds for [10000, 10000, 1368] epochs and to a loss of ['0.745', '0.756', '0.2']Trained 232 at 344.6269211769104 seconds for [10000, 10000, 1335] epochs and to a loss of ['0.745', '0.746', '0.2']Trained 236 at 379.84163308143616 seconds for [10000, 10000, 1256] epochs and to a loss of ['0.439', '0.742', '0.2']Trained 240 at 414.92352962493896 seconds for [10000, 10000, 1193] epochs and to a loss of ['0.417', '0.74', '0.2']Trained 244 at 449.9042294025421 seconds for [10000, 10000, 1182] epochs and to a loss of ['0.416', '0.739', '0.2']Trained 248 at 485.04215717315674 seconds for [10000, 10000, 1199] epochs and to a loss of ['0.415', '0.74', '0.2']Trained 252 at 520.2013049125671 seconds for [10000, 10000, 1221] epochs and to a loss of ['0.415', '0.742', '0.2']Trained 256 at 555.3172845840454 seconds for [10000, 10000, 1228] epochs and to a loss of ['0.415', '0.744', '0.2']Trained 260 at 590.3054559230804 seconds for [10000, 10000, 1220] epochs and to a loss of ['0.415', '0.746', '0.2']Trained 264 at 625.5419337749481 seconds for [10000, 10000, 1165] epochs and to a loss of ['0.415', '0.747', '0.2']Trained 268 at 660.4971816539764 seconds for [10000, 10000, 1082] epochs and to a loss of ['0.411', '0.747', '0.2']Trained 272 at 695.5732600688934 seconds for [10000, 10000, 1021] epochs and to a loss of ['0.404', '0.416', '0.2']Trained 276 at 730.2794046401978 seconds for [10000, 10000, 959] epochs and to a loss of ['0.404', '0.401', '0.2']Trained 280 at 764.9368510246277 seconds for [10000, 10000, 920] epochs and to a loss of ['0.404', '0.4', '0.2']Trained 284 at 799.6631345748901 seconds for [10000, 10000, 899] epochs and to a loss of ['0.401', '0.401', '0.2']Trained 288 at 834.2831845283508 seconds for [10000, 10000, 906] epochs and to a loss of ['0.397', '0.403', '0.2']Trained 292 at 868.70618724823 seconds for [10000, 10000, 909] epochs and to a loss of ['0.398', '0.405', '0.2']Trained 296 at 903.4509043693542 seconds for [10000, 10000, 959] epochs and to a loss of ['0.397', '0.406', '0.2']Trained 300 at 938.2244849205017 seconds for [10000, 10000, 978] epochs and to a loss of ['0.397', '0.405', '0.2']Trained 304 at 973.1238420009613 seconds for [10000, 10000, 986] epochs and to a loss of ['0.394', '0.399', '0.2']Trained 308 at 1008.0851159095764 seconds for [10000, 10000, 1003] epochs and to a loss of ['0.39', '0.385', '0.2']Trained 312 at 1030.8605151176453 seconds for [10000, 2638, 988] epochs and to a loss of ['0.389', '0.2', '0.2']Trained 316 at 1049.5658831596375 seconds for [10000, 304, 978] epochs and to a loss of ['0.389', '0.2', '0.2']Trained 320 at 1068.7311947345734 seconds for [10000, 664, 976] epochs and to a loss of ['0.389', '0.2', '0.2']Trained 324 at 1088.5805847644806 seconds for [10000, 969, 1037] epochs and to a loss of ['0.38', '0.2', '0.2']Trained 328 at 1109.0395755767822 seconds for [10000, 1196, 1053] epochs and to a loss of ['0.381', '0.2', '0.2']Trained 332 at 1129.8721549510956 seconds for [10000, 1416, 1059] epochs and to a loss of ['0.381', '0.2', '0.2']Trained 336 at 1150.8879263401031 seconds for [10000, 1557, 1081] epochs and to a loss of ['0.375', '0.2', '0.2']Trained 340 at 1158.9853730201721 seconds for [2092, 1553, 1161] epochs and to a loss of ['0.199', '0.2', '0.2']Trained 344 at 1163.8182728290558 seconds for [22, 1516, 1337] epochs and to a loss of ['0.199', '0.2', '0.2']Trained 348 at 1168.937504529953 seconds for [189, 1454, 1398] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 352 at 1174.2326979637146 seconds for [391, 1354, 1409] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 356 at 1179.6329395771027 seconds for [535, 1262, 1377] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 360 at 1184.8785667419434 seconds for [697, 1108, 1316] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 364 at 1190.5274744033813 seconds for [823, 1050, 1459] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 368 at 1196.3605127334595 seconds for [884, 986, 1587] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 372 at 1202.0963833332062 seconds for [911, 920, 1563] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 376 at 1207.587142944336 seconds for [956, 794, 1494] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 380 at 1213.0302500724792 seconds for [961, 686, 1403] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 384 at 1218.1778962612152 seconds for [994, 710, 1312] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 388 at 1223.1066086292267 seconds for [1001, 708, 1194] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 392 at 1227.8691267967224 seconds for [1024, 700, 1110] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 396 at 1232.6025803089142 seconds for [1037, 711, 1035] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 400 at 1237.1706011295319 seconds for [1055, 725, 947] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 404 at 1241.7104532718658 seconds for [1057, 742, 891] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 408 at 1246.2527470588684 seconds for [1073, 773, 839] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 412 at 1250.8765404224396 seconds for [1128, 791, 786] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 416 at 1255.5376908779144 seconds for [1182, 822, 734] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 420 at 1260.2666256427765 seconds for [1233, 818, 714] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 424 at 1265.0488939285278 seconds for [1248, 825, 749] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 428 at 1269.6787719726562 seconds for [1227, 800, 724] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 432 at 1274.1959772109985 seconds for [1213, 783, 691] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 436 at 1278.5913043022156 seconds for [1186, 752, 666] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 440 at 1282.90163230896 seconds for [1124, 762, 650] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 444 at 1286.9408853054047 seconds for [1053, 726, 621] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 448 at 1290.7339503765106 seconds for [947, 683, 599] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 452 at 1294.2551591396332 seconds for [877, 645, 575] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 456 at 1297.6453244686127 seconds for [839, 605, 548] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 460 at 1300.8499236106873 seconds for [791, 580, 527] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 464 at 1303.903212070465 seconds for [743, 567, 511] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 468 at 1306.8460083007812 seconds for [703, 553, 497] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 472 at 1309.7110676765442 seconds for [675, 518, 488] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 476 at 1312.795301437378 seconds for [677, 491, 484] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 480 at 1315.702509880066 seconds for [700, 480, 479] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 484 at 1318.5953741073608 seconds for [716, 488, 492] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 488 at 1321.4636969566345 seconds for [716, 477, 495] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 492 at 1324.370285987854 seconds for [726, 476, 500] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 496 at 1327.2575874328613 seconds for [715, 464, 522] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 500 at 1330.1142506599426 seconds for [720, 468, 523] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 504 at 1332.9516158103943 seconds for [697, 473, 543] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 508 at 1335.8006958961487 seconds for [679, 475, 504] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 512 at 1338.6677346229553 seconds for [672, 472, 531] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 516 at 1341.4299566745758 seconds for [637, 476, 533] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 520 at 1344.2883291244507 seconds for [646, 483, 528] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 524 at 1347.056027173996 seconds for [621, 490, 526] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 528 at 1349.8959426879883 seconds for [617, 519, 515] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 532 at 1352.7557382583618 seconds for [598, 565, 514] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 536 at 1355.6863927841187 seconds for [625, 596, 518] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 540 at 1358.7614619731903 seconds for [643, 628, 527] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 544 at 1361.8490705490112 seconds for [630, 657, 535] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 548 at 1364.9009308815002 seconds for [618, 635, 545] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 552 at 1368.1793866157532 seconds for [606, 733, 569] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 556 at 1371.7668116092682 seconds for [606, 795, 593] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 560 at 1375.2566847801208 seconds for [619, 806, 614] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 564 at 1378.8003232479095 seconds for [629, 820, 623] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 568 at 1382.328024148941 seconds for [643, 805, 630] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 572 at 1385.876769542694 seconds for [649, 794, 634] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 576 at 1389.3162117004395 seconds for [633, 761, 628] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 580 at 1392.5589587688446 seconds for [611, 691, 615] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 584 at 1395.6468048095703 seconds for [582, 645, 591] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 588 at 1398.6675276756287 seconds for [579, 639, 576] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 592 at 1401.6817600727081 seconds for [590, 636, 534] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 596 at 1404.6314480304718 seconds for [585, 659, 503] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 600 at 1407.5025522708893 seconds for [578, 693, 445] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 200 at 1450.1447415351868 seconds for [10000, 10000, 5244] epochs and to a loss of ['0.893', '1.161', '0.2']Trained 204 at 1483.464527606964 seconds for [10000, 10000, 473] epochs and to a loss of ['0.779', '1.106', '0.2']Trained 208 at 1516.8814158439636 seconds for [10000, 10000, 600] epochs and to a loss of ['0.754', '1.088', '0.2']Trained 212 at 1550.4098680019379 seconds for [10000, 10000, 710] epochs and to a loss of ['0.744', '1.082', '0.2']Trained 216 at 1584.0503611564636 seconds for [10000, 10000, 829] epochs and to a loss of ['0.739', '1.08', '0.2']Trained 220 at 1617.8296341896057 seconds for [10000, 10000, 941] epochs and to a loss of ['0.737', '1.079', '0.2']Trained 224 at 1651.5924272537231 seconds for [10000, 10000, 1070] epochs and to a loss of ['0.735', '1.078', '0.2']Trained 228 at 1685.833153963089 seconds for [10000, 10000, 1206] epochs and to a loss of ['0.735', '1.078', '0.2']Trained 232 at 1720.1823768615723 seconds for [10000, 10000, 1331] epochs and to a loss of ['0.733', '1.078', '0.2']Trained 236 at 1754.7311325073242 seconds for [10000, 10000, 1481] epochs and to a loss of ['0.733', '1.074', '0.2']Trained 240 at 1790.0395646095276 seconds for [10000, 10000, 1723] epochs and to a loss of ['0.733', '1.071', '0.2']Trained 244 at 1826.0061523914337 seconds for [10000, 10000, 2021] epochs and to a loss of ['0.733', '1.071', '0.2']Trained 248 at 1862.149493932724 seconds for [10000, 10000, 2277] epochs and to a loss of ['0.733', '1.071', '0.2']Trained 252 at 1898.0981516838074 seconds for [10000, 10000, 2267] epochs and to a loss of ['0.733', '1.072', '0.2']Trained 256 at 1935.0783219337463 seconds for [10000, 10000, 2725] epochs and to a loss of ['0.733', '1.072', '0.2']Trained 260 at 1972.1480379104614 seconds for [10000, 10000, 2982] epochs and to a loss of ['0.733', '1.072', '0.2']Trained 264 at 2009.3229804039001 seconds for [10000, 10000, 3032] epochs and to a loss of ['0.733', '1.073', '0.2']Trained 268 at 2046.2437732219696 seconds for [10000, 10000, 2883] epochs and to a loss of ['0.726', '1.075', '0.2']Trained 272 at 2083.5426392555237 seconds for [10000, 10000, 3024] epochs and to a loss of ['0.726', '1.076', '0.2']Trained 276 at 2121.3509006500244 seconds for [10000, 10000, 2922] epochs and to a loss of ['0.726', '1.077', '0.2']Trained 280 at 2158.729010105133 seconds for [10000, 10000, 2999] epochs and to a loss of ['0.726', '1.078', '0.2']Trained 284 at 2195.8050956726074 seconds for [10000, 10000, 2857] epochs and to a loss of ['0.727', '1.08', '0.2']Trained 288 at 2232.713900089264 seconds for [10000, 10000, 2583] epochs and to a loss of ['0.72', '1.08', '0.2']Trained 292 at 2268.6845502853394 seconds for [10000, 10000, 2275] epochs and to a loss of ['0.719', '1.08', '0.2']Trained 296 at 2304.20836353302 seconds for [10000, 10000, 1995] epochs and to a loss of ['0.713', '1.081', '0.2']Trained 300 at 2339.259440422058 seconds for [10000, 10000, 1684] epochs and to a loss of ['0.705', '1.08', '0.2']Trained 304 at 2374.0032727718353 seconds for [10000, 10000, 1436] epochs and to a loss of ['0.705', '1.081', '0.2']Trained 308 at 2409.7754323482513 seconds for [10000, 10000, 1218] epochs and to a loss of ['0.705', '1.08', '0.2']Trained 312 at 2443.906925201416 seconds for [10000, 10000, 1030] epochs and to a loss of ['0.705', '0.758', '0.2']Trained 316 at 2478.00537109375 seconds for [10000, 10000, 952] epochs and to a loss of ['0.705', '0.754', '0.2']Trained 320 at 2512.400859117508 seconds for [10000, 10000, 1283] epochs and to a loss of ['0.705', '0.753', '0.2']Trained 324 at 2547.340849876404 seconds for [10000, 10000, 1512] epochs and to a loss of ['0.706', '0.748', '0.2']Trained 328 at 2582.88738656044 seconds for [10000, 10000, 1719] epochs and to a loss of ['0.706', '0.744', '0.2']Trained 332 at 2619.155309200287 seconds for [10000, 10000, 1812] epochs and to a loss of ['0.706', '0.745', '0.2']Trained 336 at 2655.3775641918182 seconds for [10000, 10000, 1799] epochs and to a loss of ['0.706', '0.747', '0.2']Trained 340 at 2691.510021686554 seconds for [10000, 10000, 1783] epochs and to a loss of ['0.706', '0.749', '0.2']Trained 344 at 2727.893634557724 seconds for [10000, 10000, 1886] epochs and to a loss of ['0.706', '0.751', '0.2']Trained 348 at 2764.1164515018463 seconds for [10000, 10000, 1963] epochs and to a loss of ['0.706', '0.752', '0.2']Trained 352 at 2800.62141084671 seconds for [10000, 10000, 2012] epochs and to a loss of ['0.705', '0.742', '0.2']Trained 356 at 2837.3302145004272 seconds for [10000, 10000, 2126] epochs and to a loss of ['0.698', '0.738', '0.2']Trained 360 at 2874.1828413009644 seconds for [10000, 10000, 2217] epochs and to a loss of ['0.698', '0.426', '0.2']Trained 364 at 2910.8261168003082 seconds for [10000, 10000, 2229] epochs and to a loss of ['0.698', '0.42', '0.2']Trained 368 at 2947.573588371277 seconds for [10000, 10000, 2289] epochs and to a loss of ['0.698', '0.41', '0.2']Trained 372 at 2983.2689139842987 seconds for [10000, 10000, 1661] epochs and to a loss of ['0.698', '0.401', '0.2']Trained 376 at 3019.2607777118683 seconds for [10000, 10000, 1642] epochs and to a loss of ['0.698', '0.399', '0.2']Trained 380 at 3055.986129760742 seconds for [10000, 10000, 2208] epochs and to a loss of ['0.695', '0.397', '0.2']Trained 384 at 3092.954710483551 seconds for [10000, 10000, 2368] epochs and to a loss of ['0.692', '0.394', '0.2']Trained 388 at 3130.035084962845 seconds for [10000, 10000, 2248] epochs and to a loss of ['0.693', '0.386', '0.2']Trained 392 at 3166.4874126911163 seconds for [10000, 10000, 1931] epochs and to a loss of ['0.696', '0.386', '0.2']Trained 396 at 3202.180285692215 seconds for [10000, 10000, 1536] epochs and to a loss of ['0.697', '0.378', '0.2']Trained 400 at 3237.1813666820526 seconds for [10000, 10000, 1221] epochs and to a loss of ['0.698', '0.377', '0.2']Trained 404 at 3271.901342153549 seconds for [10000, 10000, 1011] epochs and to a loss of ['0.698', '0.376', '0.2']Trained 408 at 3306.741466283798 seconds for [10000, 10000, 868] epochs and to a loss of ['0.695', '0.374', '0.2']Trained 412 at 3341.044191837311 seconds for [10000, 10000, 773] epochs and to a loss of ['0.692', '0.373', '0.2']Trained 416 at 3375.2495114803314 seconds for [10000, 10000, 702] epochs and to a loss of ['0.69', '0.371', '0.2']Trained 420 at 3409.388125181198 seconds for [10000, 10000, 652] epochs and to a loss of ['0.674', '0.37', '0.2']Trained 424 at 3443.491707086563 seconds for [10000, 10000, 618] epochs and to a loss of ['0.363', '0.368', '0.2']Trained 428 at 3477.493438720703 seconds for [10000, 10000, 650] epochs and to a loss of ['0.36', '0.367', '0.2']Trained 432 at 3511.6353204250336 seconds for [10000, 10000, 725] epochs and to a loss of ['0.355', '0.366', '0.2']Trained 436 at 3530.5659255981445 seconds for [696, 10000, 765] epochs and to a loss of ['0.2', '0.365', '0.2']Trained 440 at 3548.4892971515656 seconds for [108, 10000, 789] epochs and to a loss of ['0.2', '0.365', '0.2']Trained 444 at 3566.9359424114227 seconds for [302, 10000, 819] epochs and to a loss of ['0.2', '0.364', '0.2']Trained 448 at 3585.6409800052643 seconds for [441, 10000, 808] epochs and to a loss of ['0.199', '0.365', '0.2']Trained 452 at 3604.4677379131317 seconds for [549, 10000, 775] epochs and to a loss of ['0.2', '0.365', '0.2']Trained 456 at 3623.211935043335 seconds for [589, 10000, 737] epochs and to a loss of ['0.2', '0.365', '0.2']Trained 460 at 3641.885385274887 seconds for [587, 10000, 698] epochs and to a loss of ['0.2', '0.365', '0.2']Trained 464 at 3660.491885662079 seconds for [588, 10000, 662] epochs and to a loss of ['0.2', '0.366', '0.2']Trained 468 at 3679.1316843032837 seconds for [583, 10000, 671] epochs and to a loss of ['0.2', '0.366', '0.2']Trained 472 at 3697.642091989517 seconds for [573, 10000, 639] epochs and to a loss of ['0.2', '0.367', '0.2']Trained 476 at 3715.8910348415375 seconds for [555, 10000, 599] epochs and to a loss of ['0.2', '0.367', '0.2']Trained 480 at 3734.3669385910034 seconds for [545, 10000, 580] epochs and to a loss of ['0.2', '0.368', '0.2']Trained 484 at 3752.6952266693115 seconds for [533, 10000, 586] epochs and to a loss of ['0.2', '0.369', '0.2']Trained 488 at 3771.04120016098 seconds for [515, 10000, 609] epochs and to a loss of ['0.2', '0.37', '0.2']Trained 492 at 3782.390030145645 seconds for [506, 5692, 610] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 496 at 3784.263081073761 seconds for [485, 49, 576] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 500 at 3786.172502040863 seconds for [470, 114, 543] epochs and to a loss of ['0.2', '0.199', '0.2']Trained 504 at 3788.2397043704987 seconds for [461, 232, 527] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 508 at 3790.39981174469 seconds for [464, 340, 499] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 512 at 3792.765288591385 seconds for [478, 394, 493] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 516 at 3795.182202577591 seconds for [499, 462, 480] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 520 at 3797.748167991638 seconds for [526, 519, 473] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 524 at 3800.5516395568848 seconds for [555, 557, 525] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 528 at 3803.490790128708 seconds for [573, 582, 573] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 532 at 3806.5533900260925 seconds for [593, 594, 647] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 536 at 3809.870664358139 seconds for [621, 597, 699] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 540 at 3813.2762610912323 seconds for [664, 609, 715] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 544 at 3816.6906237602234 seconds for [724, 609, 712] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 548 at 3820.221691131592 seconds for [780, 606, 703] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 552 at 3823.853558063507 seconds for [849, 597, 672] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 556 at 3827.4533729553223 seconds for [893, 582, 663] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 560 at 3831.216138124466 seconds for [968, 564, 694] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 564 at 3835.137532711029 seconds for [1042, 551, 737] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 568 at 3839.176926612854 seconds for [1078, 541, 767] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 572 at 3843.307028055191 seconds for [1118, 533, 776] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 576 at 3847.3751368522644 seconds for [1145, 530, 767] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 580 at 3851.5554122924805 seconds for [1139, 574, 736] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 584 at 3855.778948545456 seconds for [1143, 677, 703] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 588 at 3860.1875331401825 seconds for [1123, 735, 736] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 592 at 3864.8819477558136 seconds for [1134, 741, 931] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 596 at 3869.9972088336945 seconds for [1185, 719, 1129] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 600 at 3875.3445234298706 seconds for [1096, 665, 1161] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 200 at 3926.960552930832 seconds for [10000, 10000, 10000] epochs and to a loss of ['2.147', '0.608', '0.515']Trained 204 at 3975.9671421051025 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.931', '0.577', '0.429']Trained 208 at 4024.8267765045166 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.883', '0.563', '0.411']Trained 212 at 4073.742001056671 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.869', '0.558', '0.403']Trained 216 at 4122.6636798381805 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.859', '0.556', '0.399']Trained 220 at 4171.87637424469 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.86', '0.555', '0.398']Trained 224 at 4220.628146886826 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.859', '0.555', '0.397']Trained 228 at 4269.426716566086 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.855', '0.555', '0.396']Trained 232 at 4318.388196468353 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.513', '0.555', '0.396']Trained 236 at 4367.179795503616 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.49', '0.555', '0.394']Trained 240 at 4415.701488018036 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.477', '0.555', '0.392']Trained 244 at 4464.276195764542 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.468', '0.55', '0.393']Trained 248 at 4515.63819026947 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.462', '0.55', '0.394']Trained 252 at 4566.7918338775635 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.458', '0.55', '0.395']Trained 256 at 4615.514188528061 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.454', '0.551', '0.398']Trained 260 at 4664.290363788605 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.447', '0.553', '0.399']Trained 264 at 4713.083167314529 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.443', '0.555', '0.4']Trained 268 at 4761.885362863541 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.439', '0.557', '0.402']Trained 272 at 4810.9142644405365 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.419', '0.56', '0.403']Trained 276 at 4859.899314403534 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.416', '0.559', '0.404']Trained 280 at 4908.431368589401 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.413', '0.556', '0.401']Trained 284 at 4957.31981420517 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.409', '0.556', '0.403']Trained 288 at 5006.180979728699 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.404', '0.556', '0.406']Trained 292 at 5054.882670879364 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.401', '0.557', '0.41']Trained 296 at 5104.020419836044 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.4', '0.556', '0.415']Trained 300 at 5152.720575094223 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.4', '0.556', '0.422']Trained 304 at 5201.371948719025 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.399', '0.556', '0.427']Trained 308 at 5250.04088973999 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.399', '0.557', '0.43']Trained 312 at 5298.661289691925 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.398', '0.558', '0.428']Trained 316 at 5347.262189149857 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.398', '0.559', '0.42']Trained 320 at 5398.001989126205 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.069', '0.561', '0.408']Trained 324 at 5446.905385255814 seconds for [10000, 10000, 10000] epochs and to a loss of ['1.069', '0.561', '0.405']Trained 328 at 5495.504343986511 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.741', '0.561', '0.401']Trained 332 at 5544.046934127808 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.42', '0.559', '0.398']Trained 336 at 5594.773117780685 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.416', '0.557', '0.396']Trained 340 at 5640.2603549957275 seconds for [10000, 10000, 7936] epochs and to a loss of ['0.419', '0.555', '0.2']Trained 344 at 5673.074339866638 seconds for [10000, 10000, 311] epochs and to a loss of ['0.422', '0.552', '0.2']Trained 348 at 5707.044663906097 seconds for [10000, 10000, 790] epochs and to a loss of ['0.421', '0.55', '0.2']Trained 352 at 5741.749985456467 seconds for [10000, 10000, 1333] epochs and to a loss of ['0.411', '0.549', '0.2']Trained 356 at 5776.640855073929 seconds for [10000, 10000, 1440] epochs and to a loss of ['0.412', '0.548', '0.2']Trained 360 at 5811.353007316589 seconds for [10000, 10000, 1392] epochs and to a loss of ['0.412', '0.547', '0.2']Trained 364 at 5846.135397195816 seconds for [10000, 10000, 1293] epochs and to a loss of ['0.412', '0.547', '0.2']Trained 368 at 5880.591566562653 seconds for [10000, 10000, 1193] epochs and to a loss of ['0.411', '0.547', '0.2']Trained 372 at 5914.723932266235 seconds for [10000, 10000, 1108] epochs and to a loss of ['0.404', '0.547', '0.2']Trained 376 at 5948.900892496109 seconds for [10000, 10000, 1065] epochs and to a loss of ['0.396', '0.548', '0.2']Trained 380 at 5984.632287502289 seconds for [10000, 10000, 1077] epochs and to a loss of ['0.395', '0.549', '0.2']Trained 384 at 6018.853196620941 seconds for [10000, 10000, 1036] epochs and to a loss of ['0.395', '0.549', '0.2']Trained 388 at 6052.966203212738 seconds for [10000, 10000, 984] epochs and to a loss of ['0.395', '0.55', '0.2']Trained 392 at 6087.138660430908 seconds for [10000, 10000, 994] epochs and to a loss of ['0.395', '0.55', '0.2']Trained 396 at 6121.314530134201 seconds for [10000, 10000, 970] epochs and to a loss of ['0.395', '0.55', '0.2']Trained 400 at 6155.310472488403 seconds for [10000, 10000, 976] epochs and to a loss of ['0.395', '0.55', '0.2']Trained 404 at 6189.285774946213 seconds for [10000, 10000, 975] epochs and to a loss of ['0.395', '0.549', '0.2']Trained 408 at 6223.263889312744 seconds for [10000, 10000, 971] epochs and to a loss of ['0.395', '0.547', '0.2']Trained 412 at 6257.286210298538 seconds for [10000, 10000, 995] epochs and to a loss of ['0.395', '0.546', '0.2']Trained 416 at 6291.591556072235 seconds for [10000, 10000, 1016] epochs and to a loss of ['0.394', '0.545', '0.2']Trained 420 at 6325.681850671768 seconds for [10000, 10000, 1015] epochs and to a loss of ['0.392', '0.544', '0.2']Trained 424 at 6359.636564254761 seconds for [10000, 10000, 993] epochs and to a loss of ['0.39', '0.543', '0.2']Trained 428 at 6393.531489133835 seconds for [10000, 10000, 950] epochs and to a loss of ['0.388', '0.542', '0.2']Trained 432 at 6427.312092065811 seconds for [10000, 10000, 902] epochs and to a loss of ['0.381', '0.542', '0.2']Trained 436 at 6446.05752658844 seconds for [730, 10000, 843] epochs and to a loss of ['0.199', '0.542', '0.2']Trained 440 at 6463.894892454147 seconds for [264, 10000, 756] epochs and to a loss of ['0.2', '0.541', '0.2']Trained 444 at 6482.040397882462 seconds for [515, 10000, 673] epochs and to a loss of ['0.2', '0.541', '0.2']Trained 448 at 6500.174457788467 seconds for [589, 10000, 575] epochs and to a loss of ['0.2', '0.541', '0.2']Trained 452 at 6518.2099006175995 seconds for [602, 10000, 575] epochs and to a loss of ['0.2', '0.541', '0.2']Trained 456 at 6536.415855884552 seconds for [588, 10000, 583] epochs and to a loss of ['0.2', '0.54', '0.2']Trained 460 at 6554.7713577747345 seconds for [574, 10000, 614] epochs and to a loss of ['0.2', '0.541', '0.2']Trained 464 at 6572.925011873245 seconds for [578, 10000, 651] epochs and to a loss of ['0.2', '0.541', '0.2']Trained 468 at 6591.116564512253 seconds for [573, 10000, 683] epochs and to a loss of ['0.2', '0.54', '0.2']Trained 472 at 6609.545906543732 seconds for [613, 10000, 737] epochs and to a loss of ['0.2', '0.54', '0.2']Trained 476 at 6628.008350610733 seconds for [626, 10000, 793] epochs and to a loss of ['0.2', '0.54', '0.2']Trained 480 at 6646.577583789825 seconds for [645, 10000, 836] epochs and to a loss of ['0.2', '0.539', '0.2']Trained 484 at 6665.306942224503 seconds for [651, 10000, 882] epochs and to a loss of ['0.2', '0.54', '0.2']Trained 488 at 6683.92284822464 seconds for [639, 10000, 890] epochs and to a loss of ['0.2', '0.54', '0.2']Trained 492 at 6702.555569887161 seconds for [634, 10000, 895] epochs and to a loss of ['0.2', '0.54', '0.2']Trained 496 at 6721.241142749786 seconds for [634, 10000, 906] epochs and to a loss of ['0.2', '0.54', '0.2']Trained 500 at 6739.879775285721 seconds for [631, 10000, 896] epochs and to a loss of ['0.2', '0.367', '0.2']Trained 504 at 6758.602969884872 seconds for [636, 10000, 881] epochs and to a loss of ['0.2', '0.367', '0.2']Trained 508 at 6777.240881204605 seconds for [636, 10000, 862] epochs and to a loss of ['0.2', '0.367', '0.2']Trained 512 at 6795.679513931274 seconds for [629, 10000, 819] epochs and to a loss of ['0.2', '0.366', '0.2']Trained 516 at 6814.220206022263 seconds for [684, 10000, 746] epochs and to a loss of ['0.2', '0.365', '0.2']Trained 520 at 6832.904487371445 seconds for [775, 10000, 710] epochs and to a loss of ['0.2', '0.363', '0.2']Trained 524 at 6851.48482131958 seconds for [798, 10000, 701] epochs and to a loss of ['0.2', '0.363', '0.2']Trained 528 at 6870.475422620773 seconds for [790, 10000, 707] epochs and to a loss of ['0.2', '0.363', '0.2']Trained 532 at 6889.074458599091 seconds for [761, 10000, 716] epochs and to a loss of ['0.2', '0.363', '0.2']Trained 536 at 6907.573926925659 seconds for [735, 10000, 721] epochs and to a loss of ['0.2', '0.363', '0.2']Trained 540 at 6926.008841753006 seconds for [693, 10000, 726] epochs and to a loss of ['0.2', '0.363', '0.2']Trained 544 at 6944.761007070541 seconds for [678, 10000, 733] epochs and to a loss of ['0.2', '0.363', '0.2']Trained 548 at 6963.348323106766 seconds for [647, 10000, 763] epochs and to a loss of ['0.2', '0.364', '0.2']Trained 552 at 6982.100501298904 seconds for [703, 10000, 838] epochs and to a loss of ['0.2', '0.364', '0.2']Trained 556 at 7001.287153244019 seconds for [714, 10000, 1100] epochs and to a loss of ['0.2', '0.365', '0.2']Trained 560 at 7021.403812885284 seconds for [699, 10000, 1623] epochs and to a loss of ['0.2', '0.366', '0.2']Trained 564 at 7041.574372053146 seconds for [694, 10000, 1767] epochs and to a loss of ['0.2', '0.365', '0.2']Trained 568 at 7062.616827726364 seconds for [683, 10000, 2178] epochs and to a loss of ['0.2', '0.364', '0.2']Trained 572 at 7084.336475133896 seconds for [668, 10000, 2689] epochs and to a loss of ['0.2', '0.363', '0.2']Trained 576 at 7106.5094656944275 seconds for [663, 10000, 2991] epochs and to a loss of ['0.2', '0.362', '0.2']Trained 580 at 7130.878852367401 seconds for [667, 10000, 4279] epochs and to a loss of ['0.2', '0.361', '0.2']Trained 584 at 7154.79759812355 seconds for [677, 10000, 4040] epochs and to a loss of ['0.2', '0.361', '0.2']Trained 588 at 7177.295041799545 seconds for [668, 10000, 3214] epochs and to a loss of ['0.2', '0.361', '0.2']Trained 592 at 7198.984206914902 seconds for [648, 10000, 2688] epochs and to a loss of ['0.2', '0.362', '0.2']Trained 596 at 7219.929019212723 seconds for [665, 10000, 2209] epochs and to a loss of ['0.2', '0.362', '0.2']Trained 600 at 7240.778595685959 seconds for [725, 10000, 1997] epochs and to a loss of ['0.2', '0.362', '0.2']Trained 200 at 7290.3039338588715 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.779', '1.281', '1.593']Trained 204 at 7338.267162799835 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.649', '1.172', '1.48']Trained 208 at 7385.804909944534 seconds for [9939, 10000, 10000] epochs and to a loss of ['0.2', '1.15', '1.452']Trained 212 at 7417.750338792801 seconds for [335, 10000, 10000] epochs and to a loss of ['0.2', '1.139', '1.442']Trained 216 at 7449.801519155502 seconds for [448, 10000, 10000] epochs and to a loss of ['0.2', '1.133', '1.437']Trained 220 at 7482.165618419647 seconds for [562, 10000, 10000] epochs and to a loss of ['0.2', '1.129', '1.435']Trained 224 at 7514.9834856987 seconds for [714, 10000, 10000] epochs and to a loss of ['0.2', '1.123', '1.431']Trained 228 at 7548.158658027649 seconds for [839, 10000, 10000] epochs and to a loss of ['0.2', '1.122', '1.419']Trained 232 at 7581.309514045715 seconds for [919, 10000, 10000] epochs and to a loss of ['0.2', '1.123', '1.412']Trained 236 at 7614.629124403 seconds for [964, 10000, 10000] epochs and to a loss of ['0.2', '1.123', '1.411']Trained 240 at 7648.037797451019 seconds for [979, 10000, 10000] epochs and to a loss of ['0.2', '1.124', '1.41']Trained 244 at 7681.475250720978 seconds for [1010, 10000, 10000] epochs and to a loss of ['0.2', '1.126', '1.41']Trained 248 at 7714.843269109726 seconds for [1087, 10000, 10000] epochs and to a loss of ['0.2', '1.129', '1.41']Trained 252 at 7748.423200368881 seconds for [1190, 10000, 10000] epochs and to a loss of ['0.2', '1.131', '1.41']Trained 256 at 7782.417058229446 seconds for [1342, 10000, 10000] epochs and to a loss of ['0.2', '1.133', '1.41']Trained 260 at 7816.50373673439 seconds for [1544, 10000, 10000] epochs and to a loss of ['0.2', '1.135', '1.405']Trained 264 at 7851.81290268898 seconds for [1756, 10000, 10000] epochs and to a loss of ['0.2', '1.138', '1.403']Trained 268 at 7887.436518907547 seconds for [2011, 10000, 10000] epochs and to a loss of ['0.2', '1.14', '1.402']Trained 272 at 7923.4895923137665 seconds for [2160, 10000, 10000] epochs and to a loss of ['0.2', '1.142', '1.177']Trained 276 at 7959.657858133316 seconds for [2279, 10000, 10000] epochs and to a loss of ['0.2', '1.142', '1.079']Trained 280 at 7995.997798681259 seconds for [2401, 10000, 10000] epochs and to a loss of ['0.2', '1.142', '1.075']Trained 284 at 8032.389718770981 seconds for [2441, 10000, 10000] epochs and to a loss of ['0.2', '1.139', '1.073']Trained 288 at 8068.931887626648 seconds for [2515, 10000, 10000] epochs and to a loss of ['0.2', '1.137', '1.072']Trained 292 at 8105.551066637039 seconds for [2519, 10000, 10000] epochs and to a loss of ['0.2', '1.126', '1.071']Trained 296 at 8142.219681024551 seconds for [2573, 10000, 10000] epochs and to a loss of ['0.2', '1.12', '1.071']Trained 300 at 8179.484538793564 seconds for [2746, 10000, 10000] epochs and to a loss of ['0.2', '1.117', '1.071']Trained 304 at 8216.755235433578 seconds for [2841, 10000, 10000] epochs and to a loss of ['0.2', '1.115', '0.741']Trained 308 at 8254.103656768799 seconds for [2918, 10000, 10000] epochs and to a loss of ['0.2', '1.103', '0.731']Trained 312 at 8292.229256153107 seconds for [3013, 10000, 10000] epochs and to a loss of ['0.2', '1.106', '0.727']Trained 316 at 8342.871652841568 seconds for [3094, 10000, 10000] epochs and to a loss of ['0.2', '1.107', '0.72']Trained 320 at 8391.342099666595 seconds for [3166, 10000, 10000] epochs and to a loss of ['0.2', '1.089', '0.719']Trained 324 at 8439.796584367752 seconds for [3102, 10000, 10000] epochs and to a loss of ['0.2', '1.086', '0.719']Trained 328 at 8488.172086000443 seconds for [3089, 10000, 10000] epochs and to a loss of ['0.2', '1.07', '0.721']Trained 332 at 8536.197949647903 seconds for [3183, 10000, 10000] epochs and to a loss of ['0.2', '1.065', '0.723']Trained 336 at 8584.912131547928 seconds for [3445, 10000, 10000] epochs and to a loss of ['0.2', '1.061', '0.726']Trained 340 at 8634.044810295105 seconds for [3599, 10000, 10000] epochs and to a loss of ['0.2', '1.058', '0.724']Trained 344 at 8683.569152355194 seconds for [3707, 10000, 10000] epochs and to a loss of ['0.2', '1.055', '0.723']Trained 348 at 8732.86424779892 seconds for [3764, 10000, 10000] epochs and to a loss of ['0.2', '1.053', '0.719']Trained 352 at 8782.710695505142 seconds for [3871, 10000, 10000] epochs and to a loss of ['0.2', '1.052', '0.718']Trained 356 at 8832.636185884476 seconds for [3833, 10000, 10000] epochs and to a loss of ['0.2', '1.051', '0.718']Trained 360 at 8876.406193494797 seconds for [3360, 10000, 10000] epochs and to a loss of ['0.2', '0.735', '0.719']Trained 364 at 8917.730589389801 seconds for [2664, 10000, 10000] epochs and to a loss of ['0.2', '0.394', '0.721']Trained 368 at 8958.089000940323 seconds for [2051, 10000, 10000] epochs and to a loss of ['0.2', '0.39', '0.721']Trained 372 at 8999.601326227188 seconds for [1692, 10000, 10000] epochs and to a loss of ['0.2', '0.39', '0.723']Trained 376 at 9036.715326786041 seconds for [1413, 10000, 10000] epochs and to a loss of ['0.2', '0.389', '0.722']Trained 380 at 9073.928238153458 seconds for [1269, 10000, 10000] epochs and to a loss of ['0.2', '0.389', '0.722']Trained 384 at 9111.179664850235 seconds for [1215, 10000, 10000] epochs and to a loss of ['0.2', '0.388', '0.721']Trained 388 at 9148.760275363922 seconds for [1211, 10000, 10000] epochs and to a loss of ['0.2', '0.388', '0.721']Trained 392 at 9186.063230276108 seconds for [1230, 10000, 10000] epochs and to a loss of ['0.2', '0.388', '0.722']Trained 396 at 9225.073575735092 seconds for [1196, 10000, 10000] epochs and to a loss of ['0.2', '0.388', '0.721']Trained 400 at 9262.496352672577 seconds for [1156, 10000, 10000] epochs and to a loss of ['0.2', '0.388', '0.72']Trained 404 at 9300.275576353073 seconds for [1027, 10000, 10000] epochs and to a loss of ['0.2', '0.388', '0.719']Trained 408 at 9341.792299985886 seconds for [941, 10000, 10000] epochs and to a loss of ['0.2', '0.388', '0.718']Trained 412 at 9384.444013595581 seconds for [1103, 10000, 10000] epochs and to a loss of ['0.2', '0.388', '0.717']Trained 416 at 9422.673946619034 seconds for [1153, 10000, 10000] epochs and to a loss of ['0.2', '0.387', '0.715']Trained 420 at 9461.421814203262 seconds for [1145, 10000, 10000] epochs and to a loss of ['0.2', '0.387', '0.713']Trained 424 at 9499.112226486206 seconds for [1139, 10000, 10000] epochs and to a loss of ['0.2', '0.386', '0.711']Trained 428 at 9536.830837488174 seconds for [1142, 10000, 10000] epochs and to a loss of ['0.2', '0.383', '0.71']Trained 432 at 9576.226647853851 seconds for [1136, 10000, 10000] epochs and to a loss of ['0.2', '0.379', '0.702']Trained 436 at 9617.53193950653 seconds for [1126, 10000, 10000] epochs and to a loss of ['0.2', '0.379', '0.701']Trained 440 at 9655.538528442383 seconds for [1085, 10000, 10000] epochs and to a loss of ['0.2', '0.379', '0.701']Trained 444 at 9693.223489522934 seconds for [1094, 10000, 10000] epochs and to a loss of ['0.2', '0.379', '0.701']Trained 448 at 9730.682846784592 seconds for [1115, 10000, 10000] epochs and to a loss of ['0.2', '0.379', '0.701']Trained 452 at 9768.5161755085 seconds for [1118, 10000, 10000] epochs and to a loss of ['0.2', '0.379', '0.701']Trained 456 at 9805.658262968063 seconds for [1075, 10000, 10000] epochs and to a loss of ['0.2', '0.38', '0.701']Trained 460 at 9843.376068592072 seconds for [1114, 10000, 10000] epochs and to a loss of ['0.2', '0.38', '0.701']Trained 464 at 9880.964094400406 seconds for [1282, 10000, 10000] epochs and to a loss of ['0.2', '0.379', '0.7']Trained 468 at 9919.024301290512 seconds for [1431, 10000, 10000] epochs and to a loss of ['0.2', '0.372', '0.7']Trained 472 at 9957.923110723495 seconds for [1566, 10000, 10000] epochs and to a loss of ['0.2', '0.372', '0.699']Trained 476 at 9997.94091129303 seconds for [1712, 10000, 10000] epochs and to a loss of ['0.2', '0.372', '0.698']Trained 480 at 10036.906307697296 seconds for [1867, 10000, 10000] epochs and to a loss of ['0.2', '0.371', '0.698']Trained 484 at 10076.24711561203 seconds for [1984, 10000, 10000] epochs and to a loss of ['0.2', '0.371', '0.697']Trained 488 at 10115.490881204605 seconds for [2023, 10000, 10000] epochs and to a loss of ['0.2', '0.366', '0.697']Trained 492 at 10154.534160614014 seconds for [1934, 10000, 10000] epochs and to a loss of ['0.2', '0.365', '0.697']Trained 496 at 10193.835161209106 seconds for [2162, 10000, 10000] epochs and to a loss of ['0.2', '0.366', '0.697']Trained 500 at 10233.426848888397 seconds for [2211, 10000, 10000] epochs and to a loss of ['0.2', '0.366', '0.697']Trained 504 at 10273.516285657883 seconds for [2313, 10000, 10000] epochs and to a loss of ['0.2', '0.366', '0.698']Trained 508 at 10313.412192344666 seconds for [2434, 10000, 10000] epochs and to a loss of ['0.2', '0.366', '0.694']Trained 512 at 10353.657926797867 seconds for [2501, 10000, 10000] epochs and to a loss of ['0.2', '0.365', '0.696']Trained 516 at 10382.82837677002 seconds for [2466, 3940, 10000] epochs and to a loss of ['0.2', '0.199', '0.696']Trained 520 at 10405.377039670944 seconds for [2391, 141, 10000] epochs and to a loss of ['0.2', '0.2', '0.696']Trained 524 at 10427.717217445374 seconds for [2255, 189, 10000] epochs and to a loss of ['0.2', '0.199', '0.695']Trained 528 at 10449.92685675621 seconds for [2071, 232, 10000] epochs and to a loss of ['0.2', '0.2', '0.696']Trained 532 at 10471.838315725327 seconds for [1884, 285, 10000] epochs and to a loss of ['0.2', '0.2', '0.696']Trained 536 at 10493.447255373001 seconds for [1685, 355, 10000] epochs and to a loss of ['0.2', '0.2', '0.696']Trained 540 at 10514.947204113007 seconds for [1513, 388, 10000] epochs and to a loss of ['0.2', '0.2', '0.697']Trained 544 at 10535.912054777145 seconds for [1289, 404, 10000] epochs and to a loss of ['0.2', '0.2', '0.696']Trained 548 at 10556.600633144379 seconds for [1044, 436, 10000] epochs and to a loss of ['0.2', '0.2', '0.696']Trained 552 at 10576.836588144302 seconds for [800, 477, 10000] epochs and to a loss of ['0.2', '0.2', '0.695']Trained 556 at 10596.96231842041 seconds for [615, 483, 10000] epochs and to a loss of ['0.2', '0.2', '0.366']Trained 560 at 10616.819128274918 seconds for [470, 480, 10000] epochs and to a loss of ['0.2', '0.2', '0.366']Trained 564 at 10636.449018716812 seconds for [404, 477, 10000] epochs and to a loss of ['0.2', '0.2', '0.366']Trained 568 at 10655.93032002449 seconds for [422, 485, 10000] epochs and to a loss of ['0.2', '0.2', '0.366']Trained 572 at 10675.530497789383 seconds for [436, 490, 10000] epochs and to a loss of ['0.2', '0.2', '0.366']Trained 576 at 10694.865876197815 seconds for [445, 496, 10000] epochs and to a loss of ['0.2', '0.2', '0.367']Trained 580 at 10714.293194532394 seconds for [507, 498, 10000] epochs and to a loss of ['0.2', '0.2', '0.369']Trained 584 at 10733.943882703781 seconds for [576, 493, 10000] epochs and to a loss of ['0.2', '0.2', '0.371']Trained 588 at 10753.768508195877 seconds for [641, 491, 10000] epochs and to a loss of ['0.2', '0.2', '0.373']Trained 592 at 10773.479047060013 seconds for [679, 476, 10000] epochs and to a loss of ['0.2', '0.2', '0.373']Trained 596 at 10793.328588962555 seconds for [707, 470, 10000] epochs and to a loss of ['0.2', '0.2', '0.373']Trained 600 at 10813.230944633484 seconds for [730, 461, 10000] epochs and to a loss of ['0.2', '0.2', '0.373']Trained 200 at 10868.376727342606 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.485', '0.834', '0.804']Trained 204 at 10922.746685504913 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.415', '0.782', '0.754']Trained 208 at 10967.896283388138 seconds for [5111, 10000, 10000] epochs and to a loss of ['0.2', '0.768', '0.74']Trained 212 at 11004.291997432709 seconds for [185, 10000, 10000] epochs and to a loss of ['0.2', '0.759', '0.732']Trained 216 at 11041.14023065567 seconds for [272, 10000, 10000] epochs and to a loss of ['0.2', '0.749', '0.73']Trained 220 at 11077.764488697052 seconds for [335, 10000, 10000] epochs and to a loss of ['0.2', '0.743', '0.729']Trained 224 at 11114.557110786438 seconds for [388, 10000, 10000] epochs and to a loss of ['0.2', '0.741', '0.722']Trained 228 at 11151.314969062805 seconds for [441, 10000, 10000] epochs and to a loss of ['0.2', '0.74', '0.718']Trained 232 at 11188.283895254135 seconds for [470, 10000, 10000] epochs and to a loss of ['0.2', '0.739', '0.715']Trained 236 at 11225.116937875748 seconds for [513, 10000, 10000] epochs and to a loss of ['0.2', '0.738', '0.715']Trained 240 at 11261.92059135437 seconds for [578, 10000, 10000] epochs and to a loss of ['0.2', '0.738', '0.716']Trained 244 at 11299.056937217712 seconds for [630, 10000, 10000] epochs and to a loss of ['0.2', '0.736', '0.717']Trained 248 at 11336.84171462059 seconds for [704, 10000, 10000] epochs and to a loss of ['0.2', '0.73', '0.717']Trained 252 at 11374.340835571289 seconds for [780, 10000, 10000] epochs and to a loss of ['0.2', '0.73', '0.718']Trained 256 at 11411.734800815582 seconds for [878, 10000, 10000] epochs and to a loss of ['0.2', '0.73', '0.718']Trained 260 at 11450.461908578873 seconds for [1018, 10000, 10000] epochs and to a loss of ['0.2', '0.73', '0.719']Trained 264 at 11488.88230729103 seconds for [1211, 10000, 10000] epochs and to a loss of ['0.2', '0.73', '0.713']Trained 268 at 11527.69516134262 seconds for [1410, 10000, 10000] epochs and to a loss of ['0.2', '0.73', '0.715']Trained 272 at 11566.645730018616 seconds for [1563, 10000, 10000] epochs and to a loss of ['0.2', '0.73', '0.717']Trained 276 at 11605.57375741005 seconds for [1711, 10000, 10000] epochs and to a loss of ['0.2', '0.731', '0.719']Trained 280 at 11644.65099143982 seconds for [1811, 10000, 10000] epochs and to a loss of ['0.2', '0.729', '0.722']Trained 284 at 11683.860301971436 seconds for [1773, 10000, 10000] epochs and to a loss of ['0.2', '0.726', '0.725']Trained 288 at 11722.75259566307 seconds for [1717, 10000, 10000] epochs and to a loss of ['0.2', '0.727', '0.728']Trained 292 at 11761.698878765106 seconds for [1711, 10000, 10000] epochs and to a loss of ['0.2', '0.729', '0.73']Trained 296 at 11800.564894199371 seconds for [1690, 10000, 10000] epochs and to a loss of ['0.2', '0.731', '0.73']Trained 300 at 11839.572966814041 seconds for [1638, 10000, 10000] epochs and to a loss of ['0.2', '0.734', '0.409']Trained 304 at 11878.402873277664 seconds for [1576, 10000, 10000] epochs and to a loss of ['0.2', '0.737', '0.403']Trained 308 at 11916.34819817543 seconds for [1479, 10000, 10000] epochs and to a loss of ['0.2', '0.741', '0.389']Trained 312 at 11954.132863521576 seconds for [1442, 10000, 10000] epochs and to a loss of ['0.2', '0.744', '0.385']Trained 316 at 11991.980242490768 seconds for [1386, 10000, 10000] epochs and to a loss of ['0.2', '0.429', '0.382']Trained 320 at 12029.704977989197 seconds for [1374, 10000, 10000] epochs and to a loss of ['0.2', '0.429', '0.38']Trained 324 at 12067.662260055542 seconds for [1391, 10000, 10000] epochs and to a loss of ['0.2', '0.427', '0.378']Trained 328 at 12105.384166955948 seconds for [1384, 10000, 10000] epochs and to a loss of ['0.2', '0.425', '0.376']Trained 332 at 12143.241907596588 seconds for [1398, 10000, 10000] epochs and to a loss of ['0.2', '0.431', '0.374']Trained 336 at 12181.325125217438 seconds for [1461, 10000, 10000] epochs and to a loss of ['0.2', '0.438', '0.373']Trained 340 at 12219.695563316345 seconds for [1517, 10000, 10000] epochs and to a loss of ['0.2', '0.446', '0.373']Trained 344 at 12258.525333404541 seconds for [1593, 10000, 10000] epochs and to a loss of ['0.2', '0.453', '0.372']Trained 348 at 12297.0801217556 seconds for [1622, 10000, 10000] epochs and to a loss of ['0.2', '0.458', '0.371']Trained 352 at 12335.333096504211 seconds for [1575, 10000, 10000] epochs and to a loss of ['0.2', '0.46', '0.371']Trained 356 at 12373.494128227234 seconds for [1521, 10000, 10000] epochs and to a loss of ['0.2', '0.45', '0.366']Trained 360 at 12411.631266832352 seconds for [1472, 10000, 10000] epochs and to a loss of ['0.2', '0.44', '0.361']Trained 364 at 12450.168725967407 seconds for [1467, 10000, 10000] epochs and to a loss of ['0.2', '0.432', '0.358']Trained 368 at 12488.275854587555 seconds for [1458, 10000, 10000] epochs and to a loss of ['0.2', '0.425', '0.357']Trained 372 at 12525.869161367416 seconds for [1430, 10000, 10000] epochs and to a loss of ['0.2', '0.419', '0.357']Trained 376 at 12563.337515830994 seconds for [1383, 10000, 10000] epochs and to a loss of ['0.2', '0.414', '0.357']Trained 380 at 12600.700639724731 seconds for [1323, 10000, 10000] epochs and to a loss of ['0.2', '0.411', '0.354']Trained 384 at 12638.074905395508 seconds for [1237, 10000, 10000] epochs and to a loss of ['0.2', '0.409', '0.351']Trained 388 at 12675.531018257141 seconds for [1156, 10000, 10000] epochs and to a loss of ['0.2', '0.406', '0.35']Trained 392 at 12712.414262533188 seconds for [1063, 10000, 10000] epochs and to a loss of ['0.2', '0.405', '0.351']Trained 396 at 12749.497388601303 seconds for [994, 10000, 10000] epochs and to a loss of ['0.2', '0.404', '0.351']Trained 400 at 12786.516254663467 seconds for [927, 10000, 10000] epochs and to a loss of ['0.2', '0.402', '0.351']Trained 404 at 12823.638199090958 seconds for [857, 10000, 10000] epochs and to a loss of ['0.2', '0.402', '0.35']Trained 408 at 12849.92563199997 seconds for [767, 10000, 4093] epochs and to a loss of ['0.2', '0.401', '0.199']Trained 412 at 12869.151889324188 seconds for [700, 10000, 160] epochs and to a loss of ['0.2', '0.4', '0.2']Trained 416 at 12888.82433438301 seconds for [677, 10000, 414] epochs and to a loss of ['0.2', '0.4', '0.2']Trained 420 at 12909.36272740364 seconds for [659, 10000, 842] epochs and to a loss of ['0.2', '0.399', '0.2']Trained 424 at 12930.2989590168 seconds for [650, 10000, 1082] epochs and to a loss of ['0.2', '0.399', '0.2']Trained 428 at 12951.438262939453 seconds for [636, 10000, 1218] epochs and to a loss of ['0.2', '0.399', '0.2']Trained 432 at 12972.468291521072 seconds for [605, 10000, 1285] epochs and to a loss of ['0.2', '0.398', '0.2']Trained 436 at 12993.563377857208 seconds for [593, 10000, 1389] epochs and to a loss of ['0.2', '0.398', '0.2']Trained 440 at 13014.816343545914 seconds for [569, 10000, 1463] epochs and to a loss of ['0.2', '0.398', '0.2']Trained 444 at 13035.276845932007 seconds for [554, 10000, 1516] epochs and to a loss of ['0.2', '0.397', '0.2']Trained 448 at 13055.894925355911 seconds for [537, 10000, 1593] epochs and to a loss of ['0.199', '0.396', '0.2']Trained 452 at 13076.535267591476 seconds for [517, 10000, 1626] epochs and to a loss of ['0.2', '0.395', '0.2']Trained 456 at 13097.275511980057 seconds for [535, 10000, 1624] epochs and to a loss of ['0.2', '0.393', '0.2']Trained 460 at 13117.94438958168 seconds for [574, 10000, 1558] epochs and to a loss of ['0.2', '0.388', '0.2']Trained 464 at 13138.693020105362 seconds for [636, 10000, 1490] epochs and to a loss of ['0.2', '0.387', '0.2']Trained 468 at 13159.234889745712 seconds for [660, 10000, 1336] epochs and to a loss of ['0.2', '0.387', '0.2']Trained 472 at 13180.716807365417 seconds for [792, 10000, 1254] epochs and to a loss of ['0.2', '0.388', '0.2']Trained 476 at 13201.406352043152 seconds for [879, 10000, 1180] epochs and to a loss of ['0.2', '0.388', '0.2']Trained 480 at 13221.926176071167 seconds for [859, 10000, 1118] epochs and to a loss of ['0.2', '0.389', '0.2']Trained 484 at 13242.397446155548 seconds for [914, 10000, 1066] epochs and to a loss of ['0.2', '0.39', '0.2']Trained 488 at 13262.66300201416 seconds for [835, 10000, 1030] epochs and to a loss of ['0.2', '0.391', '0.2']Trained 492 at 13282.705880403519 seconds for [865, 10000, 1006] epochs and to a loss of ['0.2', '0.391', '0.2']Trained 496 at 13302.710928678513 seconds for [862, 10000, 965] epochs and to a loss of ['0.2', '0.391', '0.2']Trained 500 at 13322.719157218933 seconds for [892, 10000, 896] epochs and to a loss of ['0.2', '0.39', '0.2']Trained 504 at 13342.543872117996 seconds for [925, 10000, 828] epochs and to a loss of ['0.2', '0.389', '0.2']Trained 508 at 13362.257816076279 seconds for [953, 10000, 747] epochs and to a loss of ['0.2', '0.389', '0.2']Trained 512 at 13382.088593244553 seconds for [1083, 10000, 650] epochs and to a loss of ['0.2', '0.388', '0.2']Trained 516 at 13401.91295003891 seconds for [1171, 10000, 564] epochs and to a loss of ['0.2', '0.388', '0.2']Trained 520 at 13421.828198671341 seconds for [1143, 10000, 632] epochs and to a loss of ['0.2', '0.388', '0.2']Trained 524 at 13440.170225858688 seconds for [1077, 9069, 637] epochs and to a loss of ['0.2', '0.199', '0.2']Trained 528 at 13443.232439279556 seconds for [968, 141, 640] epochs and to a loss of ['0.2', '0.199', '0.2']Trained 532 at 13446.365131855011 seconds for [885, 286, 635] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 536 at 13449.706896066666 seconds for [816, 468, 621] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 540 at 13453.265501022339 seconds for [735, 642, 669] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 544 at 13457.196913480759 seconds for [704, 773, 766] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 548 at 13461.226191282272 seconds for [681, 796, 845] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 552 at 13465.803353786469 seconds for [781, 929, 928] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 556 at 13470.873958349228 seconds for [805, 1114, 987] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 560 at 13476.080480575562 seconds for [776, 1225, 999] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 564 at 13481.171235084534 seconds for [705, 1239, 981] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 568 at 13486.078792333603 seconds for [619, 1205, 1012] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 572 at 13490.86128950119 seconds for [684, 1152, 939] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 576 at 13495.992146015167 seconds for [1036, 1086, 837] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 580 at 13501.423246145248 seconds for [1307, 1043, 772] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 584 at 13506.65010380745 seconds for [1324, 994, 690] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 588 at 13511.520101070404 seconds for [1255, 936, 609] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 592 at 13516.083781719208 seconds for [1199, 893, 564] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 596 at 13520.43423128128 seconds for [1159, 841, 509] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 600 at 13524.830267190933 seconds for [1328, 740, 453] epochs and to a loss of ['0.2', '0.2', '0.2']