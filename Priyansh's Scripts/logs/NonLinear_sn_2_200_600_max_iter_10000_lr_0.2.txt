Trained 200 at 43.88611674308777 seconds for [10000, 3675, 10000] epochs and to a loss of ['0.792', '0.2', '0.413']Trained 204 at 75.48167133331299 seconds for [10000, 282, 10000] epochs and to a loss of ['0.777', '0.2', '0.301']Trained 208 at 105.97264885902405 seconds for [10000, 365, 10000] epochs and to a loss of ['0.447', '0.2', '0.275']Trained 212 at 139.0910725593567 seconds for [10000, 413, 10000] epochs and to a loss of ['0.443', '0.2', '0.261']Trained 216 at 167.81942415237427 seconds for [6918, 446, 10000] epochs and to a loss of ['0.2', '0.2', '0.255']Trained 220 at 184.71893167495728 seconds for [163, 442, 10000] epochs and to a loss of ['0.2', '0.2', '0.251']Trained 224 at 201.6005561351776 seconds for [247, 437, 10000] epochs and to a loss of ['0.2', '0.2', '0.247']Trained 228 at 219.52510690689087 seconds for [335, 484, 10000] epochs and to a loss of ['0.2', '0.2', '0.245']Trained 232 at 236.63430261611938 seconds for [403, 519, 10000] epochs and to a loss of ['0.2', '0.2', '0.244']Trained 236 at 254.15351510047913 seconds for [445, 531, 10000] epochs and to a loss of ['0.2', '0.2', '0.244']Trained 240 at 271.4986960887909 seconds for [479, 540, 10000] epochs and to a loss of ['0.2', '0.2', '0.244']Trained 244 at 289.00405979156494 seconds for [494, 562, 10000] epochs and to a loss of ['0.2', '0.2', '0.244']Trained 248 at 306.41384863853455 seconds for [516, 574, 10000] epochs and to a loss of ['0.2', '0.2', '0.244']Trained 252 at 323.91116285324097 seconds for [515, 608, 10000] epochs and to a loss of ['0.2', '0.2', '0.245']Trained 256 at 341.4184432029724 seconds for [525, 676, 10000] epochs and to a loss of ['0.2', '0.2', '0.246']Trained 260 at 359.0208809375763 seconds for [534, 769, 10000] epochs and to a loss of ['0.2', '0.2', '0.247']Trained 264 at 376.85660886764526 seconds for [550, 878, 10000] epochs and to a loss of ['0.2', '0.2', '0.248']Trained 268 at 394.848424911499 seconds for [558, 969, 10000] epochs and to a loss of ['0.2', '0.2', '0.251']Trained 272 at 413.1183702945709 seconds for [563, 1066, 10000] epochs and to a loss of ['0.2', '0.2', '0.254']Trained 276 at 431.45034742355347 seconds for [568, 1162, 10000] epochs and to a loss of ['0.2', '0.2', '0.253']Trained 280 at 450.0718638896942 seconds for [583, 1325, 10000] epochs and to a loss of ['0.2', '0.2', '0.252']Trained 284 at 469.07001662254333 seconds for [613, 1493, 10000] epochs and to a loss of ['0.2', '0.2', '0.251']Trained 288 at 488.3746156692505 seconds for [635, 1689, 10000] epochs and to a loss of ['0.2', '0.2', '0.25']Trained 292 at 508.2501699924469 seconds for [649, 2052, 10000] epochs and to a loss of ['0.2', '0.2', '0.248']Trained 296 at 529.1783554553986 seconds for [672, 2496, 10000] epochs and to a loss of ['0.2', '0.2', '0.244']Trained 300 at 550.27676820755 seconds for [718, 2734, 10000] epochs and to a loss of ['0.2', '0.2', '0.243']Trained 304 at 571.7333233356476 seconds for [733, 2887, 10000] epochs and to a loss of ['0.2', '0.2', '0.243']Trained 308 at 593.2105791568756 seconds for [769, 3105, 10000] epochs and to a loss of ['0.2', '0.2', '0.243']Trained 312 at 614.9648292064667 seconds for [787, 3325, 10000] epochs and to a loss of ['0.2', '0.2', '0.243']Trained 316 at 637.3174221515656 seconds for [782, 3588, 10000] epochs and to a loss of ['0.2', '0.2', '0.244']Trained 320 at 659.9251856803894 seconds for [780, 3784, 10000] epochs and to a loss of ['0.2', '0.2', '0.244']Trained 324 at 682.1973810195923 seconds for [769, 3643, 10000] epochs and to a loss of ['0.2', '0.2', '0.246']Trained 328 at 703.3326771259308 seconds for [744, 2932, 10000] epochs and to a loss of ['0.2', '0.2', '0.251']Trained 332 at 723.2912456989288 seconds for [721, 2228, 10000] epochs and to a loss of ['0.2', '0.2', '0.25']Trained 336 at 742.7324295043945 seconds for [675, 1925, 10000] epochs and to a loss of ['0.2', '0.2', '0.25']Trained 340 at 761.8908319473267 seconds for [669, 1766, 10000] epochs and to a loss of ['0.2', '0.2', '0.25']Trained 344 at 780.6008276939392 seconds for [689, 1459, 10000] epochs and to a loss of ['0.2', '0.2', '0.249']Trained 348 at 801.2338497638702 seconds for [676, 1236, 10000] epochs and to a loss of ['0.2', '0.2', '0.25']Trained 352 at 820.3032865524292 seconds for [678, 1100, 10000] epochs and to a loss of ['0.2', '0.2', '0.25']Trained 356 at 838.3722870349884 seconds for [676, 990, 10000] epochs and to a loss of ['0.2', '0.2', '0.251']Trained 360 at 856.4034521579742 seconds for [688, 933, 10000] epochs and to a loss of ['0.2', '0.2', '0.252']Trained 364 at 874.5789771080017 seconds for [697, 860, 10000] epochs and to a loss of ['0.2', '0.2', '0.253']Trained 368 at 892.4386928081512 seconds for [711, 836, 10000] epochs and to a loss of ['0.2', '0.2', '0.254']Trained 372 at 910.4020204544067 seconds for [695, 817, 10000] epochs and to a loss of ['0.2', '0.2', '0.253']Trained 376 at 928.8040840625763 seconds for [641, 799, 10000] epochs and to a loss of ['0.2', '0.2', '0.253']Trained 380 at 946.9551911354065 seconds for [679, 836, 10000] epochs and to a loss of ['0.2', '0.2', '0.252']Trained 384 at 965.2321650981903 seconds for [689, 902, 10000] epochs and to a loss of ['0.2', '0.2', '0.252']Trained 388 at 983.4524216651917 seconds for [642, 959, 10000] epochs and to a loss of ['0.2', '0.2', '0.251']Trained 392 at 1001.6814930438995 seconds for [581, 1010, 10000] epochs and to a loss of ['0.2', '0.2', '0.251']Trained 396 at 1020.0657184123993 seconds for [518, 1061, 10000] epochs and to a loss of ['0.2', '0.2', '0.251']Trained 400 at 1039.7463247776031 seconds for [447, 1066, 10000] epochs and to a loss of ['0.2', '0.2', '0.25']Trained 404 at 1061.3180935382843 seconds for [378, 1077, 10000] epochs and to a loss of ['0.2', '0.2', '0.25']Trained 408 at 1082.7640364170074 seconds for [339, 1077, 10000] epochs and to a loss of ['0.2', '0.2', '0.25']Trained 412 at 1103.0229127407074 seconds for [350, 1028, 10000] epochs and to a loss of ['0.2', '0.2', '0.242']Trained 416 at 1123.3651173114777 seconds for [343, 976, 10000] epochs and to a loss of ['0.2', '0.2', '0.241']Trained 420 at 1143.8879861831665 seconds for [326, 915, 10000] epochs and to a loss of ['0.2', '0.2', '0.241']Trained 424 at 1164.3860747814178 seconds for [326, 852, 10000] epochs and to a loss of ['0.2', '0.2', '0.241']Trained 428 at 1184.8162033557892 seconds for [358, 780, 10000] epochs and to a loss of ['0.2', '0.2', '0.241']Trained 432 at 1205.2401723861694 seconds for [362, 717, 10000] epochs and to a loss of ['0.2', '0.2', '0.241']Trained 436 at 1225.2719144821167 seconds for [373, 682, 10000] epochs and to a loss of ['0.2', '0.2', '0.241']Trained 440 at 1247.1038239002228 seconds for [364, 650, 10000] epochs and to a loss of ['0.2', '0.2', '0.24']Trained 444 at 1267.538625240326 seconds for [383, 604, 10000] epochs and to a loss of ['0.2', '0.2', '0.24']Trained 448 at 1287.9890475273132 seconds for [393, 570, 10000] epochs and to a loss of ['0.2', '0.2', '0.24']Trained 452 at 1310.1275844573975 seconds for [414, 540, 10000] epochs and to a loss of ['0.2', '0.2', '0.24']Trained 456 at 1329.028566122055 seconds for [419, 538, 10000] epochs and to a loss of ['0.2', '0.2', '0.24']Trained 460 at 1350.925740480423 seconds for [427, 535, 10000] epochs and to a loss of ['0.2', '0.2', '0.24']Trained 464 at 1372.1089868545532 seconds for [446, 541, 10000] epochs and to a loss of ['0.2', '0.2', '0.24']Trained 468 at 1391.5687456130981 seconds for [445, 551, 10000] epochs and to a loss of ['0.2', '0.2', '0.241']Trained 472 at 1414.5841524600983 seconds for [448, 564, 10000] epochs and to a loss of ['0.2', '0.2', '0.241']Trained 476 at 1433.251101732254 seconds for [447, 577, 10000] epochs and to a loss of ['0.2', '0.2', '0.235']Trained 480 at 1450.8852939605713 seconds for [436, 596, 10000] epochs and to a loss of ['0.2', '0.2', '0.233']Trained 484 at 1469.4830448627472 seconds for [425, 612, 10000] epochs and to a loss of ['0.2', '0.2', '0.233']Trained 488 at 1489.6768336296082 seconds for [392, 610, 10000] epochs and to a loss of ['0.2', '0.2', '0.233']Trained 492 at 1508.2074007987976 seconds for [411, 791, 10000] epochs and to a loss of ['0.2', '0.2', '0.233']Trained 496 at 1528.8459479808807 seconds for [413, 844, 10000] epochs and to a loss of ['0.2', '0.2', '0.232']Trained 500 at 1548.4357521533966 seconds for [427, 1035, 10000] epochs and to a loss of ['0.199', '0.2', '0.226']Trained 504 at 1569.4206535816193 seconds for [444, 1273, 10000] epochs and to a loss of ['0.2', '0.2', '0.225']Trained 508 at 1588.6350803375244 seconds for [468, 1280, 10000] epochs and to a loss of ['0.2', '0.2', '0.225']Trained 512 at 1609.3113415241241 seconds for [474, 1215, 10000] epochs and to a loss of ['0.2', '0.2', '0.225']Trained 516 at 1629.0332384109497 seconds for [539, 1180, 10000] epochs and to a loss of ['0.2', '0.2', '0.225']Trained 520 at 1650.124608039856 seconds for [580, 1161, 10000] epochs and to a loss of ['0.2', '0.2', '0.224']Trained 524 at 1673.2245285511017 seconds for [632, 1151, 10000] epochs and to a loss of ['0.2', '0.2', '0.224']Trained 528 at 1694.6965775489807 seconds for [656, 1049, 10000] epochs and to a loss of ['0.2', '0.2', '0.224']Trained 532 at 1704.0885376930237 seconds for [661, 1093, 3161] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 536 at 1708.1600739955902 seconds for [674, 1124, 65] epochs and to a loss of ['0.2', '0.2', '0.199']Trained 540 at 1711.8468036651611 seconds for [686, 1162, 87] epochs and to a loss of ['0.2', '0.2', '0.199']Trained 544 at 1715.013105392456 seconds for [668, 1133, 182] epochs and to a loss of ['0.2', '0.2', '0.199']Trained 548 at 1719.2077169418335 seconds for [657, 1121, 230] epochs and to a loss of ['0.2', '0.2', '0.199']Trained 552 at 1723.3732478618622 seconds for [666, 1110, 282] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 556 at 1727.5870757102966 seconds for [643, 1120, 333] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 560 at 1731.8228406906128 seconds for [589, 1096, 379] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 564 at 1736.116128206253 seconds for [596, 1074, 422] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 568 at 1740.5338757038116 seconds for [624, 1041, 451] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 572 at 1744.8768723011017 seconds for [625, 1010, 454] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 576 at 1749.5481519699097 seconds for [652, 1058, 526] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 580 at 1754.1848313808441 seconds for [736, 1031, 597] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 584 at 1759.0210361480713 seconds for [820, 1079, 596] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 588 at 1764.3807427883148 seconds for [919, 1147, 667] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 592 at 1771.0689401626587 seconds for [1044, 1603, 765] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 596 at 1778.8648374080658 seconds for [1173, 1943, 840] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 600 at 1787.4952008724213 seconds for [1287, 1922, 880] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 200 at 1849.718147277832 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.468', '0.758', '0.546']Trained 204 at 1909.2824561595917 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.414', '0.743', '0.439']Trained 208 at 1964.4450135231018 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.404', '0.731', '0.419']Trained 212 at 2021.8367280960083 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.399', '0.729', '0.412']Trained 216 at 2073.1923339366913 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.398', '0.727', '0.41']Trained 220 at 2125.65855383873 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.389', '0.727', '0.409']Trained 224 at 2177.7039091587067 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.388', '0.721', '0.408']Trained 228 at 2235.8647623062134 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.387', '0.72', '0.402']Trained 232 at 2290.860595703125 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.381', '0.72', '0.402']Trained 236 at 2341.8459057807922 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.38', '0.72', '0.403']Trained 240 at 2393.047084569931 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.38', '0.721', '0.404']Trained 244 at 2441.5527770519257 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.38', '0.721', '0.405']Trained 248 at 2489.090916156769 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.38', '0.722', '0.406']Trained 252 at 2541.0875113010406 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.381', '0.722', '0.408']Trained 256 at 2594.053472995758 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.381', '0.723', '0.41']Trained 260 at 2646.6550126075745 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.381', '0.724', '0.412']Trained 264 at 2699.8599824905396 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.382', '0.725', '0.413']Trained 268 at 2753.033210992813 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.381', '0.723', '0.412']Trained 272 at 2805.615660429001 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.381', '0.723', '0.409']Trained 276 at 2853.0981414318085 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.381', '0.726', '0.404']Trained 280 at 2901.0867309570312 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.381', '0.729', '0.4']Trained 284 at 2954.7301642894745 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.38', '0.733', '0.402']Trained 288 at 2996.1169095039368 seconds for [3332, 10000, 10000] epochs and to a loss of ['0.198', '0.737', '0.398']Trained 292 at 3031.707770586014 seconds for [90, 10000, 10000] epochs and to a loss of ['0.2', '0.742', '0.396']Trained 296 at 3068.3974435329437 seconds for [161, 10000, 10000] epochs and to a loss of ['0.199', '0.744', '0.394']Trained 300 at 3108.9956686496735 seconds for [255, 10000, 10000] epochs and to a loss of ['0.2', '0.745', '0.386']Trained 304 at 3146.9887063503265 seconds for [406, 10000, 10000] epochs and to a loss of ['0.199', '0.74', '0.386']Trained 308 at 3184.040830373764 seconds for [493, 10000, 10000] epochs and to a loss of ['0.2', '0.738', '0.386']Trained 312 at 3220.7387342453003 seconds for [556, 10000, 10000] epochs and to a loss of ['0.2', '0.737', '0.387']Trained 316 at 3257.510795354843 seconds for [585, 10000, 10000] epochs and to a loss of ['0.2', '0.736', '0.389']Trained 320 at 3294.5749883651733 seconds for [591, 10000, 10000] epochs and to a loss of ['0.2', '0.734', '0.39']Trained 324 at 3331.4297342300415 seconds for [618, 10000, 10000] epochs and to a loss of ['0.2', '0.733', '0.39']Trained 328 at 3368.6121909618378 seconds for [665, 10000, 10000] epochs and to a loss of ['0.2', '0.731', '0.389']Trained 332 at 3405.5086286067963 seconds for [704, 10000, 10000] epochs and to a loss of ['0.2', '0.728', '0.386']Trained 336 at 3443.0126781463623 seconds for [730, 10000, 10000] epochs and to a loss of ['0.2', '0.726', '0.384']Trained 340 at 3483.804754972458 seconds for [728, 10000, 10000] epochs and to a loss of ['0.2', '0.725', '0.382']Trained 344 at 3518.992264509201 seconds for [755, 10000, 10000] epochs and to a loss of ['0.2', '0.723', '0.38']Trained 348 at 3552.4545714855194 seconds for [779, 10000, 10000] epochs and to a loss of ['0.2', '0.722', '0.379']Trained 352 at 3591.0493471622467 seconds for [822, 10000, 10000] epochs and to a loss of ['0.2', '0.722', '0.378']Trained 356 at 3626.271111726761 seconds for [856, 10000, 10000] epochs and to a loss of ['0.2', '0.721', '0.377']Trained 360 at 3663.2812378406525 seconds for [831, 10000, 10000] epochs and to a loss of ['0.2', '0.721', '0.376']Trained 364 at 3698.910842895508 seconds for [806, 10000, 10000] epochs and to a loss of ['0.2', '0.721', '0.367']Trained 368 at 3734.987987279892 seconds for [795, 10000, 10000] epochs and to a loss of ['0.2', '0.723', '0.367']Trained 372 at 3768.484710454941 seconds for [767, 10000, 10000] epochs and to a loss of ['0.2', '0.725', '0.367']Trained 376 at 3805.0326974391937 seconds for [728, 10000, 10000] epochs and to a loss of ['0.2', '0.729', '0.366']Trained 380 at 3838.6216852664948 seconds for [678, 10000, 10000] epochs and to a loss of ['0.2', '0.41', '0.367']Trained 384 at 3870.713568210602 seconds for [619, 10000, 10000] epochs and to a loss of ['0.2', '0.404', '0.366']Trained 388 at 3902.986316680908 seconds for [553, 10000, 10000] epochs and to a loss of ['0.2', '0.4', '0.366']Trained 392 at 3935.2119913101196 seconds for [540, 10000, 10000] epochs and to a loss of ['0.2', '0.392', '0.366']Trained 396 at 3967.2109026908875 seconds for [537, 10000, 10000] epochs and to a loss of ['0.2', '0.391', '0.366']Trained 400 at 3999.251106262207 seconds for [538, 10000, 10000] epochs and to a loss of ['0.2', '0.389', '0.366']Trained 404 at 4031.532801628113 seconds for [602, 10000, 10000] epochs and to a loss of ['0.2', '0.388', '0.366']Trained 408 at 4064.2499997615814 seconds for [646, 10000, 10000] epochs and to a loss of ['0.2', '0.386', '0.366']Trained 412 at 4097.429999113083 seconds for [689, 10000, 10000] epochs and to a loss of ['0.2', '0.385', '0.367']Trained 416 at 4129.65518450737 seconds for [710, 10000, 10000] epochs and to a loss of ['0.2', '0.385', '0.366']Trained 420 at 4161.970422267914 seconds for [691, 10000, 10000] epochs and to a loss of ['0.2', '0.385', '0.367']Trained 424 at 4194.384517431259 seconds for [669, 10000, 10000] epochs and to a loss of ['0.2', '0.385', '0.367']Trained 428 at 4226.497684955597 seconds for [678, 10000, 10000] epochs and to a loss of ['0.2', '0.386', '0.369']Trained 432 at 4258.671286344528 seconds for [659, 10000, 10000] epochs and to a loss of ['0.2', '0.386', '0.373']Trained 436 at 4290.8161725997925 seconds for [642, 10000, 10000] epochs and to a loss of ['0.2', '0.387', '0.373']Trained 440 at 4322.686505794525 seconds for [646, 10000, 10000] epochs and to a loss of ['0.2', '0.381', '0.373']Trained 444 at 4354.337307929993 seconds for [641, 9825, 10000] epochs and to a loss of ['0.2', '0.199', '0.372']Trained 448 at 4370.944491863251 seconds for [623, 84, 10000] epochs and to a loss of ['0.2', '0.2', '0.373']Trained 452 at 4387.641844511032 seconds for [614, 200, 10000] epochs and to a loss of ['0.2', '0.2', '0.373']Trained 456 at 4404.563688755035 seconds for [603, 319, 10000] epochs and to a loss of ['0.2', '0.199', '0.374']Trained 460 at 4421.706899166107 seconds for [590, 481, 10000] epochs and to a loss of ['0.2', '0.2', '0.375']Trained 464 at 4439.167922973633 seconds for [581, 703, 10000] epochs and to a loss of ['0.2', '0.2', '0.377']Trained 468 at 4456.732043266296 seconds for [563, 790, 10000] epochs and to a loss of ['0.2', '0.2', '0.38']Trained 472 at 4474.528911352158 seconds for [602, 896, 10000] epochs and to a loss of ['0.2', '0.2', '0.381']Trained 476 at 4492.521544218063 seconds for [674, 979, 10000] epochs and to a loss of ['0.2', '0.2', '0.382']Trained 480 at 4510.6330280303955 seconds for [726, 1016, 10000] epochs and to a loss of ['0.2', '0.2', '0.383']Trained 484 at 4528.979789972305 seconds for [780, 1085, 10000] epochs and to a loss of ['0.2', '0.2', '0.389']Trained 488 at 4547.651677131653 seconds for [842, 1177, 10000] epochs and to a loss of ['0.2', '0.2', '0.389']Trained 492 at 4566.587334394455 seconds for [958, 1311, 10000] epochs and to a loss of ['0.2', '0.2', '0.384']Trained 496 at 4586.3542740345 seconds for [1115, 1610, 10000] epochs and to a loss of ['0.2', '0.2', '0.381']Trained 500 at 4606.995954275131 seconds for [1269, 2001, 10000] epochs and to a loss of ['0.2', '0.2', '0.38']Trained 504 at 4627.97115945816 seconds for [1343, 2221, 10000] epochs and to a loss of ['0.2', '0.2', '0.38']Trained 508 at 4649.2947154045105 seconds for [1414, 2299, 10000] epochs and to a loss of ['0.2', '0.2', '0.38']Trained 512 at 4670.546308517456 seconds for [1393, 2282, 10000] epochs and to a loss of ['0.2', '0.2', '0.38']Trained 516 at 4691.476492881775 seconds for [1367, 2119, 10000] epochs and to a loss of ['0.2', '0.2', '0.379']Trained 520 at 4700.687699317932 seconds for [1259, 1845, 2790] epochs and to a loss of ['0.2', '0.2', '0.198']Trained 524 at 4705.121739625931 seconds for [1118, 1622, 78] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 528 at 4709.606240510941 seconds for [972, 1476, 396] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 532 at 4714.590320110321 seconds for [855, 1362, 910] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 536 at 4719.467195272446 seconds for [759, 1273, 1024] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 540 at 4724.081333637238 seconds for [684, 1160, 1069] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 544 at 4728.503114461899 seconds for [598, 1143, 1067] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 548 at 4732.976462125778 seconds for [578, 1216, 1042] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 552 at 4737.572654247284 seconds for [596, 1331, 992] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 556 at 4742.17177772522 seconds for [614, 1360, 949] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 560 at 4746.880648851395 seconds for [624, 1434, 902] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 564 at 4751.457674980164 seconds for [619, 1418, 862] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 568 at 4756.180584192276 seconds for [649, 1517, 813] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 572 at 4760.897178649902 seconds for [706, 1548, 751] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 576 at 4765.592278718948 seconds for [751, 1536, 704] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 580 at 4770.199501514435 seconds for [773, 1502, 670] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 584 at 4774.895277023315 seconds for [775, 1539, 646] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 588 at 4779.381884098053 seconds for [771, 1435, 640] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 592 at 4783.859857559204 seconds for [742, 1424, 690] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 596 at 4789.916984081268 seconds for [689, 1566, 863] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 600 at 4795.664523601532 seconds for [661, 1741, 1165] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 200 at 4836.252620458603 seconds for [5351, 10000, 10000] epochs and to a loss of ['0.2', '0.964', '0.532']Trained 204 at 4867.898225784302 seconds for [115, 10000, 10000] epochs and to a loss of ['0.2', '0.918', '0.439']Trained 208 at 4899.004854917526 seconds for [191, 10000, 10000] epochs and to a loss of ['0.2', '0.906', '0.421']Trained 212 at 4930.119413614273 seconds for [213, 10000, 10000] epochs and to a loss of ['0.2', '0.903', '0.414']Trained 216 at 4961.4231498241425 seconds for [256, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.411']Trained 220 at 4992.648237705231 seconds for [306, 10000, 10000] epochs and to a loss of ['0.2', '0.9', '0.409']Trained 224 at 5023.890846252441 seconds for [345, 10000, 10000] epochs and to a loss of ['0.2', '0.9', '0.408']Trained 228 at 5055.355352163315 seconds for [383, 10000, 10000] epochs and to a loss of ['0.2', '0.9', '0.407']Trained 232 at 5087.511946439743 seconds for [415, 10000, 10000] epochs and to a loss of ['0.2', '0.9', '0.407']Trained 236 at 5118.90803861618 seconds for [434, 10000, 10000] epochs and to a loss of ['0.2', '0.9', '0.406']Trained 240 at 5151.558171987534 seconds for [440, 10000, 10000] epochs and to a loss of ['0.2', '0.9', '0.406']Trained 244 at 5183.354992389679 seconds for [424, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.406']Trained 248 at 5214.936678886414 seconds for [391, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.406']Trained 252 at 5246.420109033585 seconds for [361, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.406']Trained 256 at 5277.915547132492 seconds for [334, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.406']Trained 260 at 5309.346004009247 seconds for [313, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.406']Trained 264 at 5340.785198688507 seconds for [297, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.405']Trained 268 at 5372.120765924454 seconds for [284, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.405']Trained 272 at 5403.475010871887 seconds for [287, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.405']Trained 276 at 5434.906928062439 seconds for [310, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.404']Trained 280 at 5466.440058469772 seconds for [328, 10000, 10000] epochs and to a loss of ['0.2', '0.902', '0.404']Trained 284 at 5498.006907463074 seconds for [340, 10000, 10000] epochs and to a loss of ['0.2', '0.902', '0.403']Trained 288 at 5529.56374502182 seconds for [346, 10000, 10000] epochs and to a loss of ['0.2', '0.902', '0.402']Trained 292 at 5561.1541702747345 seconds for [344, 10000, 10000] epochs and to a loss of ['0.2', '0.903', '0.394']Trained 296 at 5592.734548091888 seconds for [344, 10000, 10000] epochs and to a loss of ['0.2', '0.901', '0.391']Trained 300 at 5624.2957491874695 seconds for [333, 10000, 10000] epochs and to a loss of ['0.2', '0.896', '0.39']Trained 304 at 5658.6748769283295 seconds for [333, 10000, 10000] epochs and to a loss of ['0.2', '0.896', '0.389']Trained 308 at 5690.1265461444855 seconds for [331, 10000, 10000] epochs and to a loss of ['0.2', '0.896', '0.388']Trained 312 at 5721.376068115234 seconds for [331, 10000, 10000] epochs and to a loss of ['0.2', '0.896', '0.388']Trained 316 at 5752.969500303268 seconds for [326, 10000, 10000] epochs and to a loss of ['0.2', '0.89', '0.388']Trained 320 at 5784.194797515869 seconds for [315, 10000, 10000] epochs and to a loss of ['0.2', '0.888', '0.388']Trained 324 at 5815.351846456528 seconds for [319, 10000, 10000] epochs and to a loss of ['0.2', '0.888', '0.388']Trained 328 at 5846.40997505188 seconds for [316, 10000, 10000] epochs and to a loss of ['0.2', '0.889', '0.388']Trained 332 at 5877.60230755806 seconds for [311, 10000, 10000] epochs and to a loss of ['0.2', '0.89', '0.388']Trained 336 at 5908.904495477676 seconds for [304, 10000, 10000] epochs and to a loss of ['0.2', '0.89', '0.388']Trained 340 at 5940.259915113449 seconds for [296, 10000, 10000] epochs and to a loss of ['0.2', '0.89', '0.388']Trained 344 at 5971.546719551086 seconds for [291, 10000, 10000] epochs and to a loss of ['0.2', '0.89', '0.388']Trained 348 at 6002.79302573204 seconds for [287, 10000, 10000] epochs and to a loss of ['0.2', '0.89', '0.386']Trained 352 at 6034.075917482376 seconds for [288, 10000, 10000] epochs and to a loss of ['0.2', '0.89', '0.382']Trained 356 at 6065.777989149094 seconds for [291, 10000, 10000] epochs and to a loss of ['0.2', '0.889', '0.382']Trained 360 at 6097.023136377335 seconds for [293, 10000, 10000] epochs and to a loss of ['0.2', '0.888', '0.382']Trained 364 at 6128.257018566132 seconds for [296, 10000, 10000] epochs and to a loss of ['0.2', '0.88', '0.382']Trained 368 at 6159.4763350486755 seconds for [281, 10000, 10000] epochs and to a loss of ['0.2', '0.88', '0.382']Trained 372 at 6190.769419431686 seconds for [300, 10000, 10000] epochs and to a loss of ['0.2', '0.88', '0.382']Trained 376 at 6221.906150341034 seconds for [315, 10000, 10000] epochs and to a loss of ['0.2', '0.881', '0.382']Trained 380 at 6253.0858952999115 seconds for [326, 10000, 10000] epochs and to a loss of ['0.2', '0.881', '0.382']Trained 384 at 6284.38768863678 seconds for [339, 10000, 10000] epochs and to a loss of ['0.2', '0.881', '0.382']Trained 388 at 6315.67410492897 seconds for [347, 10000, 10000] epochs and to a loss of ['0.2', '0.88', '0.382']Trained 392 at 6347.215372085571 seconds for [353, 10000, 10000] epochs and to a loss of ['0.2', '0.879', '0.382']Trained 396 at 6378.51389169693 seconds for [359, 10000, 10000] epochs and to a loss of ['0.2', '0.873', '0.382']Trained 400 at 6410.100645780563 seconds for [363, 10000, 10000] epochs and to a loss of ['0.2', '0.871', '0.382']Trained 404 at 6441.33224105835 seconds for [364, 10000, 10000] epochs and to a loss of ['0.2', '0.871', '0.383']Trained 408 at 6472.66134428978 seconds for [363, 10000, 10000] epochs and to a loss of ['0.2', '0.871', '0.382']Trained 412 at 6503.9318697452545 seconds for [337, 10000, 10000] epochs and to a loss of ['0.2', '0.871', '0.382']Trained 416 at 6535.179154157639 seconds for [349, 10000, 10000] epochs and to a loss of ['0.2', '0.872', '0.382']Trained 420 at 6566.426533460617 seconds for [345, 10000, 10000] epochs and to a loss of ['0.2', '0.874', '0.381']Trained 424 at 6597.585107564926 seconds for [338, 10000, 10000] epochs and to a loss of ['0.2', '0.875', '0.377']Trained 428 at 6628.788648843765 seconds for [327, 10000, 10000] epochs and to a loss of ['0.2', '0.877', '0.366']Trained 432 at 6660.054730892181 seconds for [326, 10000, 10000] epochs and to a loss of ['0.2', '0.882', '0.365']Trained 436 at 6691.33780503273 seconds for [323, 10000, 10000] epochs and to a loss of ['0.2', '0.885', '0.366']Trained 440 at 6722.589893579483 seconds for [336, 10000, 10000] epochs and to a loss of ['0.2', '0.888', '0.365']Trained 444 at 6753.858529090881 seconds for [362, 10000, 10000] epochs and to a loss of ['0.2', '0.891', '0.365']Trained 448 at 6785.224923849106 seconds for [388, 10000, 10000] epochs and to a loss of ['0.2', '0.895', '0.365']Trained 452 at 6816.67896437645 seconds for [406, 10000, 10000] epochs and to a loss of ['0.2', '0.9', '0.364']Trained 456 at 6848.09187746048 seconds for [423, 10000, 10000] epochs and to a loss of ['0.2', '0.906', '0.364']Trained 460 at 6879.600829124451 seconds for [431, 10000, 10000] epochs and to a loss of ['0.199', '0.912', '0.362']Trained 464 at 6910.905144691467 seconds for [430, 10000, 10000] epochs and to a loss of ['0.2', '0.913', '0.361']Trained 468 at 6942.279475688934 seconds for [433, 10000, 10000] epochs and to a loss of ['0.2', '0.904', '0.36']Trained 472 at 6974.131234407425 seconds for [428, 10000, 10000] epochs and to a loss of ['0.2', '0.728', '0.361']Trained 476 at 7005.61841750145 seconds for [413, 10000, 10000] epochs and to a loss of ['0.2', '0.724', '0.361']Trained 480 at 7037.089145898819 seconds for [410, 10000, 10000] epochs and to a loss of ['0.2', '0.721', '0.361']Trained 484 at 7068.571188211441 seconds for [406, 10000, 10000] epochs and to a loss of ['0.2', '0.72', '0.36']Trained 488 at 7099.8987159729 seconds for [389, 10000, 10000] epochs and to a loss of ['0.2', '0.719', '0.361']Trained 492 at 7131.293183803558 seconds for [377, 10000, 10000] epochs and to a loss of ['0.2', '0.717', '0.361']Trained 496 at 7162.542453527451 seconds for [382, 10000, 10000] epochs and to a loss of ['0.2', '0.717', '0.361']Trained 500 at 7193.846668958664 seconds for [419, 10000, 10000] epochs and to a loss of ['0.2', '0.716', '0.361']Trained 504 at 7225.239491701126 seconds for [446, 10000, 10000] epochs and to a loss of ['0.2', '0.715', '0.361']Trained 508 at 7256.705363273621 seconds for [445, 10000, 10000] epochs and to a loss of ['0.2', '0.714', '0.362']Trained 512 at 7288.141728878021 seconds for [459, 10000, 10000] epochs and to a loss of ['0.2', '0.713', '0.362']Trained 516 at 7319.635447740555 seconds for [467, 10000, 10000] epochs and to a loss of ['0.2', '0.704', '0.363']Trained 520 at 7351.17111992836 seconds for [480, 10000, 10000] epochs and to a loss of ['0.2', '0.376', '0.366']Trained 524 at 7382.719837665558 seconds for [497, 10000, 10000] epochs and to a loss of ['0.2', '0.375', '0.368']Trained 528 at 7414.135890960693 seconds for [519, 10000, 10000] epochs and to a loss of ['0.2', '0.375', '0.369']Trained 532 at 7445.6322321891785 seconds for [547, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.37']Trained 536 at 7477.129280567169 seconds for [576, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.372']Trained 540 at 7508.749030351639 seconds for [603, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.375']Trained 544 at 7540.528359651566 seconds for [643, 10000, 10000] epochs and to a loss of ['0.2', '0.375', '0.376']Trained 548 at 7572.359348535538 seconds for [689, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.376']Trained 552 at 7604.339413642883 seconds for [742, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.376']Trained 556 at 7636.381993055344 seconds for [790, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.376']Trained 560 at 7668.396597862244 seconds for [800, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.375']Trained 564 at 7700.444091558456 seconds for [851, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.375']Trained 568 at 7732.806156158447 seconds for [933, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.374']Trained 572 at 7765.189871311188 seconds for [1025, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.374']Trained 576 at 7797.7396941185 seconds for [1131, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.374']Trained 580 at 7830.44455075264 seconds for [1190, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.374']Trained 584 at 7863.360709905624 seconds for [1187, 10000, 10000] epochs and to a loss of ['0.2', '0.375', '0.373']Trained 588 at 7895.85365653038 seconds for [1056, 10000, 10000] epochs and to a loss of ['0.2', '0.374', '0.369']Trained 592 at 7928.065438270569 seconds for [906, 10000, 10000] epochs and to a loss of ['0.2', '0.361', '0.366']Trained 596 at 7960.038161993027 seconds for [737, 10000, 10000] epochs and to a loss of ['0.2', '0.358', '0.365']Trained 600 at 7991.792011976242 seconds for [635, 10000, 10000] epochs and to a loss of ['0.2', '0.352', '0.365']Trained 200 at 8029.360182523727 seconds for [10000, 4352, 8763] epochs and to a loss of ['1.202', '0.2', '0.2']Trained 204 at 8046.912138462067 seconds for [10000, 673, 619] epochs and to a loss of ['1.105', '0.2', '0.2']Trained 208 at 8064.72257065773 seconds for [10000, 728, 777] epochs and to a loss of ['1.085', '0.2', '0.2']Trained 212 at 8082.732916593552 seconds for [10000, 756, 872] epochs and to a loss of ['1.077', '0.2', '0.2']Trained 216 at 8100.869565010071 seconds for [10000, 763, 944] epochs and to a loss of ['1.073', '0.2', '0.2']Trained 220 at 8119.048276424408 seconds for [10000, 772, 982] epochs and to a loss of ['1.07', '0.2', '0.2']Trained 224 at 8137.134664297104 seconds for [10000, 780, 994] epochs and to a loss of ['1.069', '0.2', '0.2']Trained 228 at 8155.30318236351 seconds for [10000, 805, 966] epochs and to a loss of ['1.068', '0.2', '0.2']Trained 232 at 8173.414328813553 seconds for [10000, 851, 921] epochs and to a loss of ['1.067', '0.2', '0.2']Trained 236 at 8191.544960021973 seconds for [10000, 917, 854] epochs and to a loss of ['1.067', '0.2', '0.2']Trained 240 at 8209.857420444489 seconds for [10000, 986, 766] epochs and to a loss of ['1.067', '0.2', '0.2']Trained 244 at 8228.0601375103 seconds for [10000, 1065, 689] epochs and to a loss of ['1.067', '0.2', '0.2']Trained 248 at 8246.334415197372 seconds for [10000, 1128, 625] epochs and to a loss of ['1.067', '0.2', '0.2']Trained 252 at 8264.615233421326 seconds for [10000, 1185, 590] epochs and to a loss of ['1.067', '0.2', '0.2']Trained 256 at 8282.888028383255 seconds for [10000, 1228, 571] epochs and to a loss of ['1.067', '0.2', '0.2']Trained 260 at 8301.194569826126 seconds for [10000, 1233, 557] epochs and to a loss of ['1.068', '0.2', '0.2']Trained 264 at 8319.375940799713 seconds for [10000, 1195, 530] epochs and to a loss of ['1.068', '0.2', '0.2']Trained 268 at 8337.504658460617 seconds for [10000, 1112, 561] epochs and to a loss of ['1.069', '0.2', '0.2']Trained 272 at 8355.470474004745 seconds for [10000, 1024, 558] epochs and to a loss of ['1.069', '0.2', '0.2']Trained 276 at 8373.244216442108 seconds for [10000, 917, 568] epochs and to a loss of ['1.069', '0.2', '0.2']Trained 280 at 8390.88971042633 seconds for [10000, 816, 584] epochs and to a loss of ['1.07', '0.2', '0.2']Trained 284 at 8408.504989147186 seconds for [10000, 739, 607] epochs and to a loss of ['1.054', '0.2', '0.2']Trained 288 at 8426.102247476578 seconds for [10000, 683, 646] epochs and to a loss of ['0.741', '0.2', '0.2']Trained 292 at 8444.000540971756 seconds for [10000, 654, 714] epochs and to a loss of ['0.742', '0.2', '0.2']Trained 296 at 8461.76271033287 seconds for [10000, 644, 761] epochs and to a loss of ['0.748', '0.2', '0.2']Trained 300 at 8479.610394001007 seconds for [10000, 645, 824] epochs and to a loss of ['0.738', '0.2', '0.2']Trained 304 at 8497.564169883728 seconds for [10000, 673, 888] epochs and to a loss of ['0.718', '0.2', '0.2']Trained 308 at 8515.59077334404 seconds for [10000, 712, 926] epochs and to a loss of ['0.711', '0.2', '0.2']Trained 312 at 8533.731115341187 seconds for [10000, 728, 948] epochs and to a loss of ['0.392', '0.2', '0.2']Trained 316 at 8551.871810674667 seconds for [10000, 764, 928] epochs and to a loss of ['0.39', '0.2', '0.2']Trained 320 at 8570.270929574966 seconds for [10000, 806, 955] epochs and to a loss of ['0.387', '0.2', '0.2']Trained 324 at 8588.97058558464 seconds for [10000, 856, 1217] epochs and to a loss of ['0.38', '0.2', '0.2']Trained 328 at 8608.168885469437 seconds for [10000, 934, 1394] epochs and to a loss of ['0.379', '0.2', '0.2']Trained 332 at 8627.378450155258 seconds for [10000, 1010, 1386] epochs and to a loss of ['0.379', '0.2', '0.2']Trained 336 at 8646.638732910156 seconds for [10000, 1058, 1309] epochs and to a loss of ['0.379', '0.2', '0.2']Trained 340 at 8665.71559214592 seconds for [10000, 1071, 1220] epochs and to a loss of ['0.379', '0.2', '0.2']Trained 344 at 8685.2143805027 seconds for [10000, 1120, 1412] epochs and to a loss of ['0.379', '0.2', '0.2']Trained 348 at 8705.944049358368 seconds for [10000, 1281, 2064] epochs and to a loss of ['0.379', '0.2', '0.2']Trained 352 at 8728.197002410889 seconds for [10000, 1361, 2555] epochs and to a loss of ['0.378', '0.2', '0.2']Trained 356 at 8751.151134967804 seconds for [10000, 1375, 2583] epochs and to a loss of ['0.378', '0.2', '0.2']Trained 360 at 8775.158731937408 seconds for [10000, 1304, 2645] epochs and to a loss of ['0.375', '0.2', '0.2']Trained 364 at 8797.474675416946 seconds for [10000, 1213, 2586] epochs and to a loss of ['0.371', '0.2', '0.2']Trained 368 at 8818.344463825226 seconds for [10000, 1161, 2261] epochs and to a loss of ['0.37', '0.2', '0.2']Trained 372 at 8838.362935781479 seconds for [10000, 1052, 1841] epochs and to a loss of ['0.37', '0.2', '0.2']Trained 376 at 8857.966363191605 seconds for [10000, 985, 1626] epochs and to a loss of ['0.369', '0.2', '0.2']Trained 380 at 8876.964386463165 seconds for [10000, 928, 1289] epochs and to a loss of ['0.369', '0.2', '0.2']Trained 384 at 8895.634517669678 seconds for [10000, 877, 1152] epochs and to a loss of ['0.368', '0.2', '0.2']Trained 388 at 8914.492000341415 seconds for [10000, 819, 1327] epochs and to a loss of ['0.36', '0.2', '0.2']Trained 392 at 8933.58861875534 seconds for [10000, 785, 1480] epochs and to a loss of ['0.36', '0.2', '0.2']Trained 396 at 8952.71226143837 seconds for [10000, 805, 1503] epochs and to a loss of ['0.36', '0.2', '0.2']Trained 400 at 8971.826575517654 seconds for [10000, 831, 1487] epochs and to a loss of ['0.36', '0.2', '0.2']Trained 404 at 8990.892617464066 seconds for [10000, 900, 1383] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 408 at 9009.966811656952 seconds for [10000, 993, 1325] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 412 at 9029.398573875427 seconds for [10000, 1076, 1418] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 416 at 9049.056584596634 seconds for [10000, 1124, 1468] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 420 at 9068.56540632248 seconds for [10000, 1142, 1428] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 424 at 9088.146974802017 seconds for [10000, 1232, 1377] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 428 at 9107.672890901566 seconds for [10000, 1318, 1316] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 432 at 9127.230366706848 seconds for [10000, 1355, 1254] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 436 at 9146.788559675217 seconds for [10000, 1396, 1205] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 440 at 9166.153835058212 seconds for [10000, 1372, 1113] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 444 at 9185.388441324234 seconds for [10000, 1340, 1063] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 448 at 9204.548236608505 seconds for [10000, 1291, 1028] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 452 at 9223.451789617538 seconds for [10000, 1215, 954] epochs and to a loss of ['0.359', '0.2', '0.2']Trained 456 at 9242.064047574997 seconds for [10000, 1129, 838] epochs and to a loss of ['0.358', '0.2', '0.2']Trained 460 at 9260.597943782806 seconds for [10000, 1101, 861] epochs and to a loss of ['0.351', '0.2', '0.2']Trained 464 at 9279.36012005806 seconds for [10000, 1164, 873] epochs and to a loss of ['0.352', '0.2', '0.2']Trained 468 at 9298.250669002533 seconds for [10000, 1328, 867] epochs and to a loss of ['0.352', '0.2', '0.2']Trained 472 at 9317.33355140686 seconds for [10000, 1406, 907] epochs and to a loss of ['0.353', '0.2', '0.2']Trained 476 at 9336.482815504074 seconds for [10000, 1410, 973] epochs and to a loss of ['0.353', '0.2', '0.2']Trained 480 at 9355.966687440872 seconds for [10000, 1411, 1066] epochs and to a loss of ['0.353', '0.2', '0.2']Trained 484 at 9375.425796985626 seconds for [10000, 1391, 1142] epochs and to a loss of ['0.348', '0.2', '0.2']Trained 488 at 9394.940045595169 seconds for [10000, 1411, 1155] epochs and to a loss of ['0.35', '0.2', '0.2']Trained 492 at 9414.431071043015 seconds for [10000, 1405, 1129] epochs and to a loss of ['0.354', '0.2', '0.2']Trained 496 at 9433.746022701263 seconds for [10000, 1355, 1076] epochs and to a loss of ['0.358', '0.2', '0.2']Trained 500 at 9452.898398399353 seconds for [10000, 1336, 990] epochs and to a loss of ['0.361', '0.2', '0.2']Trained 504 at 9471.913657665253 seconds for [10000, 1318, 911] epochs and to a loss of ['0.362', '0.2', '0.2']Trained 508 at 9490.793768644333 seconds for [10000, 1297, 879] epochs and to a loss of ['0.363', '0.2', '0.2']Trained 512 at 9509.628034353256 seconds for [10000, 1237, 902] epochs and to a loss of ['0.366', '0.2', '0.2']Trained 516 at 9528.454545021057 seconds for [10000, 1201, 913] epochs and to a loss of ['0.37', '0.2', '0.2']Trained 520 at 9547.32530593872 seconds for [10000, 1147, 987] epochs and to a loss of ['0.374', '0.2', '0.2']Trained 524 at 9566.340825080872 seconds for [10000, 1146, 1050] epochs and to a loss of ['0.374', '0.2', '0.2']Trained 528 at 9585.35939359665 seconds for [10000, 1191, 1057] epochs and to a loss of ['0.374', '0.2', '0.2']Trained 532 at 9604.638352632523 seconds for [10000, 1376, 1044] epochs and to a loss of ['0.374', '0.2', '0.2']Trained 536 at 9624.498116254807 seconds for [10000, 1674, 1062] epochs and to a loss of ['0.375', '0.2', '0.2']Trained 540 at 9644.797421455383 seconds for [10000, 1925, 1113] epochs and to a loss of ['0.377', '0.2', '0.2']Trained 544 at 9665.348841190338 seconds for [10000, 1958, 1126] epochs and to a loss of ['0.376', '0.2', '0.2']Trained 548 at 9685.396554231644 seconds for [10000, 1845, 1150] epochs and to a loss of ['0.375', '0.2', '0.2']Trained 552 at 9705.359505653381 seconds for [10000, 1800, 1163] epochs and to a loss of ['0.374', '0.2', '0.2']Trained 556 at 9725.446604251862 seconds for [10000, 1759, 1152] epochs and to a loss of ['0.375', '0.2', '0.2']Trained 560 at 9745.501783132553 seconds for [10000, 1765, 1144] epochs and to a loss of ['0.377', '0.2', '0.2']Trained 564 at 9765.648181915283 seconds for [10000, 1777, 1140] epochs and to a loss of ['0.377', '0.2', '0.2']Trained 568 at 9774.949590206146 seconds for [2924, 1827, 1164] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 572 at 9780.161369800568 seconds for [142, 1872, 1215] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 576 at 9785.583543777466 seconds for [358, 1849, 1241] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 580 at 9791.438320159912 seconds for [659, 1825, 1236] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 584 at 9797.936910867691 seconds for [882, 1996, 1232] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 588 at 9804.382722854614 seconds for [1044, 1989, 1079] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 592 at 9810.766550540924 seconds for [1125, 1882, 1044] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 596 at 9817.357330083847 seconds for [1158, 1700, 1315] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 600 at 9824.203073501587 seconds for [1198, 1467, 1680] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 200 at 9872.078905820847 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.441', '1.166', '0.47']Trained 204 at 9918.35775089264 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.399', '1.089', '0.411']Trained 208 at 9964.687659978867 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.39', '1.07', '0.394']Trained 212 at 10010.659330368042 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.386', '0.791', '0.388']Trained 216 at 10056.698753118515 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.384', '0.726', '0.38']Trained 220 at 10102.817878723145 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.38', '0.72', '0.377']Trained 224 at 10148.89144897461 seconds for [10000, 10000, 10000] epochs and to a loss of ['0.372', '0.717', '0.376']Trained 228 at 10181.475679397583 seconds for [1194, 10000, 10000] epochs and to a loss of ['0.2', '0.716', '0.375']Trained 232 at 10212.542640447617 seconds for [224, 10000, 10000] epochs and to a loss of ['0.2', '0.715', '0.375']Trained 236 at 10243.858211040497 seconds for [339, 10000, 10000] epochs and to a loss of ['0.2', '0.715', '0.375']Trained 240 at 10275.342764854431 seconds for [442, 10000, 10000] epochs and to a loss of ['0.199', '0.715', '0.375']Trained 244 at 10307.014993429184 seconds for [545, 10000, 10000] epochs and to a loss of ['0.2', '0.714', '0.348']Trained 248 at 10324.128050804138 seconds for [689, 10000, 264] epochs and to a loss of ['0.2', '0.714', '0.2']Trained 252 at 10340.944811582565 seconds for [834, 10000, 119] epochs and to a loss of ['0.2', '0.714', '0.2']Trained 256 at 10358.132514238358 seconds for [957, 10000, 190] epochs and to a loss of ['0.2', '0.714', '0.2']Trained 260 at 10375.535825967789 seconds for [1066, 10000, 237] epochs and to a loss of ['0.2', '0.714', '0.2']Trained 264 at 10393.275957107544 seconds for [1169, 10000, 298] epochs and to a loss of ['0.2', '0.389', '0.2']Trained 268 at 10411.325367927551 seconds for [1350, 10000, 344] epochs and to a loss of ['0.2', '0.385', '0.2']Trained 272 at 10429.673776865005 seconds for [1548, 10000, 400] epochs and to a loss of ['0.2', '0.385', '0.2']Trained 276 at 10448.369485378265 seconds for [1728, 10000, 465] epochs and to a loss of ['0.2', '0.384', '0.2']Trained 280 at 10467.420723199844 seconds for [1841, 10000, 504] epochs and to a loss of ['0.2', '0.377', '0.2']Trained 284 at 10486.562077999115 seconds for [1882, 10000, 552] epochs and to a loss of ['0.2', '0.377', '0.2']Trained 288 at 10505.69378209114 seconds for [1852, 10000, 599] epochs and to a loss of ['0.2', '0.377', '0.2']Trained 292 at 10524.85709118843 seconds for [1793, 10000, 668] epochs and to a loss of ['0.2', '0.377', '0.2']Trained 296 at 10544.064791679382 seconds for [1695, 10000, 769] epochs and to a loss of ['0.2', '0.378', '0.2']Trained 300 at 10563.408264636993 seconds for [1555, 10000, 919] epochs and to a loss of ['0.2', '0.378', '0.2']Trained 304 at 10582.568842887878 seconds for [1378, 10000, 1060] epochs and to a loss of ['0.2', '0.378', '0.2']Trained 308 at 10601.698573350906 seconds for [1220, 10000, 1168] epochs and to a loss of ['0.2', '0.378', '0.2']Trained 312 at 10620.637394189835 seconds for [1065, 10000, 1263] epochs and to a loss of ['0.2', '0.377', '0.2']Trained 316 at 10639.512393712997 seconds for [959, 10000, 1324] epochs and to a loss of ['0.2', '0.377', '0.2']Trained 320 at 10658.462666034698 seconds for [911, 10000, 1353] epochs and to a loss of ['0.2', '0.377', '0.2']Trained 324 at 10677.216819763184 seconds for [847, 10000, 1338] epochs and to a loss of ['0.2', '0.375', '0.2']Trained 328 at 10695.76745057106 seconds for [772, 10000, 1289] epochs and to a loss of ['0.2', '0.368', '0.2']Trained 332 at 10714.212574243546 seconds for [717, 10000, 1198] epochs and to a loss of ['0.2', '0.369', '0.2']Trained 336 at 10732.301580429077 seconds for [721, 10000, 1041] epochs and to a loss of ['0.2', '0.368', '0.2']Trained 340 at 10750.384466171265 seconds for [711, 10000, 1016] epochs and to a loss of ['0.2', '0.368', '0.2']Trained 344 at 10768.308118343353 seconds for [662, 10000, 1002] epochs and to a loss of ['0.2', '0.368', '0.2']Trained 348 at 10786.08529496193 seconds for [601, 10000, 966] epochs and to a loss of ['0.2', '0.368', '0.2']Trained 352 at 10803.687776088715 seconds for [543, 10000, 908] epochs and to a loss of ['0.2', '0.368', '0.2']Trained 356 at 10821.149023532867 seconds for [530, 10000, 865] epochs and to a loss of ['0.2', '0.368', '0.2']Trained 360 at 10838.653239250183 seconds for [522, 10000, 840] epochs and to a loss of ['0.2', '0.369', '0.2']Trained 364 at 10856.098765850067 seconds for [507, 10000, 799] epochs and to a loss of ['0.2', '0.37', '0.2']Trained 368 at 10873.44977235794 seconds for [497, 10000, 766] epochs and to a loss of ['0.2', '0.371', '0.2']Trained 372 at 10890.719895601273 seconds for [502, 10000, 716] epochs and to a loss of ['0.2', '0.37', '0.2']Trained 376 at 10907.928678512573 seconds for [510, 10000, 678] epochs and to a loss of ['0.2', '0.365', '0.2']Trained 380 at 10925.163509607315 seconds for [543, 10000, 645] epochs and to a loss of ['0.2', '0.366', '0.2']Trained 384 at 10942.383827447891 seconds for [612, 10000, 593] epochs and to a loss of ['0.2', '0.366', '0.2']Trained 388 at 10959.715363740921 seconds for [642, 10000, 573] epochs and to a loss of ['0.2', '0.367', '0.2']Trained 392 at 10976.916942358017 seconds for [600, 10000, 560] epochs and to a loss of ['0.2', '0.367', '0.2']Trained 396 at 10994.149214744568 seconds for [628, 10000, 553] epochs and to a loss of ['0.2', '0.369', '0.2']Trained 400 at 11011.421847343445 seconds for [666, 10000, 563] epochs and to a loss of ['0.2', '0.371', '0.2']Trained 404 at 11028.824467658997 seconds for [683, 10000, 594] epochs and to a loss of ['0.2', '0.373', '0.2']Trained 408 at 11046.310236692429 seconds for [702, 10000, 679] epochs and to a loss of ['0.2', '0.376', '0.2']Trained 412 at 11064.14001584053 seconds for [733, 10000, 792] epochs and to a loss of ['0.2', '0.378', '0.2']Trained 416 at 11082.07470035553 seconds for [767, 10000, 852] epochs and to a loss of ['0.2', '0.377', '0.2']Trained 420 at 11099.948179721832 seconds for [770, 10000, 817] epochs and to a loss of ['0.2', '0.378', '0.2']Trained 424 at 11115.514385700226 seconds for [838, 8398, 771] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 428 at 11118.40218782425 seconds for [900, 226, 719] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 432 at 11121.597463846207 seconds for [1004, 377, 666] epochs and to a loss of ['0.2', '0.199', '0.2']Trained 436 at 11125.344772100449 seconds for [1078, 654, 644] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 440 at 11130.621727705002 seconds for [1108, 1599, 652] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 444 at 11136.501494169235 seconds for [1120, 1962, 666] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 448 at 11142.11854505539 seconds for [1165, 1706, 659] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 452 at 11147.418053388596 seconds for [1208, 1414, 762] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 456 at 11152.44999408722 seconds for [1260, 1146, 795] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 460 at 11157.303436756134 seconds for [1257, 992, 813] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 464 at 11162.125992774963 seconds for [1340, 906, 819] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 468 at 11166.954138994217 seconds for [1368, 842, 868] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 472 at 11171.886030435562 seconds for [1419, 809, 916] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 476 at 11176.754246711731 seconds for [1410, 764, 905] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 480 at 11181.47464632988 seconds for [1405, 705, 883] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 484 at 11185.992994070053 seconds for [1362, 654, 860] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 488 at 11190.445048809052 seconds for [1280, 606, 956] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 492 at 11194.923182725906 seconds for [1181, 575, 1080] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 496 at 11199.248089551926 seconds for [1057, 564, 1140] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 500 at 11203.393338441849 seconds for [950, 566, 1138] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 504 at 11207.265733003616 seconds for [836, 547, 1105] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 508 at 11210.946470975876 seconds for [766, 565, 1031] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 512 at 11214.362033605576 seconds for [678, 573, 907] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 516 at 11217.623625516891 seconds for [621, 608, 829] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 520 at 11220.964921236038 seconds for [577, 753, 793] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 524 at 11224.756464481354 seconds for [592, 1059, 764] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 528 at 11228.846582889557 seconds for [606, 1251, 742] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 532 at 11232.90551161766 seconds for [621, 1309, 659] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 536 at 11236.84546303749 seconds for [629, 1301, 574] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 540 at 11241.282524108887 seconds for [654, 1585, 542] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 544 at 11246.463456869125 seconds for [676, 2068, 564] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 548 at 11252.220670700073 seconds for [697, 2375, 592] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 552 at 11258.266945838928 seconds for [720, 2562, 582] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 556 at 11264.469305753708 seconds for [738, 2677, 546] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 560 at 11270.75214099884 seconds for [749, 2654, 592] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 564 at 11277.21371269226 seconds for [769, 2660, 682] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 568 at 11283.968662261963 seconds for [784, 2743, 786] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 572 at 11291.4248316288 seconds for [1032, 2838, 896] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 576 at 11299.065464258194 seconds for [1097, 2779, 1004] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 580 at 11306.723756313324 seconds for [1083, 2719, 1083] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 584 at 11314.223242044449 seconds for [1088, 2500, 1171] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 588 at 11321.48653268814 seconds for [1012, 2307, 1343] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 592 at 11328.211797714233 seconds for [895, 1893, 1495] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 596 at 11333.957221031189 seconds for [769, 1426, 1482] epochs and to a loss of ['0.2', '0.2', '0.2']Trained 600 at 11339.315749883652 seconds for [690, 1125, 1595] epochs and to a loss of ['0.2', '0.2', '0.2']